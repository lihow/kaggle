{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import imgaug as ia\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from natsort import natsorted\n",
    "import matplotlib.pyplot as plt\n",
    "from imgaug import augmenters as iaa\n",
    "from imgaug import parameters as iap\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread, imsave\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from skimage import transform, filters, exposure\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2int = {'airplane':0, 'automobile':1, 'bird':2, 'cat':3, 'deer':4, 'dog':5, 'frog':6, 'horse':7, 'ship':8, 'truck':9}\n",
    "int2lable = {0:'airplane', 1:'automobile', 2:'bird', 3:'cat', 4:'deer', 5:'dog', 6:'frog', 7:'horse', 8:'ship', 9:'truck'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#图片数据持久化，保存到本地，供下次直接调用 由于测试集比较大，本次只抽取训练集\n",
    "def data_preprocessing(data_path, resize=True, img_rows=32, img_cols=32):\n",
    "    if (os.path.exists(data_path + '/' + 'train_' + str(img_rows) +  '_' + str(img_cols) + '.npy') \n",
    "        and os.path.exists(data_path + '/' + 'test_' + str(img_rows) +  '_' + str(img_cols) + '.npy') \n",
    "        and os.path.exists(data_path + '/' + 'labels.npy')):\n",
    "        print('data have already processed')\n",
    "    else:\n",
    "        ### Image preprocessing ###\n",
    "        if resize == True:\n",
    "            if not os.path.exists(data_path + \"/trainResized\"):\n",
    "                os.makedirs(data_path + \"/trainResized\")\n",
    "            if not os.path.exists(data_path + \"/testResized\"):\n",
    "                os.makedirs(data_path + \"/testResized\")\n",
    "        for set_type in ['train', 'test']:\n",
    "            files = natsorted(glob.glob(data_path + '/' + set_type + '/*'))\n",
    "            data = np.zeros((len(files), img_rows, img_cols, 3))\n",
    "            for i, file_path in enumerate(files):\n",
    "                '''\n",
    "                img = imread(file_path, as_grey=True) #读入的图为[0, 1]图\n",
    "                img_resized = resize(img, (img_rows, img_cols))\n",
    "                data[i] = img_resized\n",
    "                #Save image\n",
    "                new_name = \"/\".join(file_path.split(\"/\")[:-1] ) + \"Resized/\" + file_path.split(\"/\")[-1]\n",
    "                imsave(new_name, img_resized)\n",
    "                '''\n",
    "                #利用opencv读取图片\n",
    "                img = cv2.imread(file_path) #读入彩色图\n",
    "                if resize == True:\n",
    "                    img_resized = cv2.resize(img, (img_rows, img_cols))#读入的[0, 255]的图\n",
    "                    data[i] = img_resized\n",
    "                    #Save image\n",
    "                    new_name = \"/\".join(file_path.split(\"/\")[:-1] ) + \"Resized/\" + file_path.split(\"/\")[-1]\n",
    "                    cv2.imwrite(new_name, img_resized) \n",
    "                else:\n",
    "                    data[i] = img\n",
    "            #Add channel/filter dimension [222, 32, 32] => [222, 1, 32, 32]\n",
    "            #train_img = np.stack(train_img)[..., None]\n",
    "            #data = data[:, :, :, np.newaxis]\n",
    "            data = data.astype('float32')\n",
    "            #data /= 255\n",
    "            np.save(data_path + '/' + set_type + '_' + str(img_rows) +  '_' + str(img_cols) + '.npy', data)\n",
    "        ### Labels preprocessing ###\n",
    "        y_train = pd.read_csv(data_path + '/trainLabels.csv').values[:, 1]\n",
    "        #Convert one-hot vectors\n",
    "        Y_train = np.zeros((y_train.shape[0], len(np.unique(y_train))))\n",
    "        for i in range(y_train.shape[0]):\n",
    "            Y_train[i][label2int[y_train[i]]] = 1\n",
    "        np.save(data_path + '/' + 'labels.npy', Y_train)\n",
    "    X_train_all = np.load(data_path + '/' + 'train_' + str(img_rows) +  '_' + str(img_cols) + '.npy')\n",
    "    Y_train_all = np.load(data_path + '/' + 'labels.npy')\n",
    "    test_all = np.load(data_path + '/' + 'test_' + str(img_rows) +  '_' + str(img_cols) + '.npy')\n",
    "    print('Finish')\n",
    "    return X_train_all, Y_train_all, test_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ba1446ce3601>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_preprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_rows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-7c905d96d9ff>\u001b[0m in \u001b[0;36mdata_preprocessing\u001b[0;34m(data_path, resize, img_rows, img_cols)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mset_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnatsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mset_type\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 '''\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_path = 'data'\n",
    "X_train_all, Y_train_all, test_all = data_preprocessing(data_path, resize=True, img_rows=224, img_cols=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_all.shape, Y_train_all.shape)\n",
    "sample_id = 6\n",
    "sample_x = X_train_all[sample_id]\n",
    "print(X_train_all[sample_id].shape)\n",
    "print(int2lable[np.argmax(Y_train_all[sample_id], axis=0)])\n",
    "plt.title('train sample', size=16)\n",
    "plt.imshow(sample_x[..., 0])\n",
    "print(X_train_all.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 划分验证集 ###\n",
    "#打乱顺序\n",
    "num_example=X_train_all.shape[0]\n",
    "arr=np.arange(num_example)\n",
    "np.random.shuffle(arr)\n",
    "X_train_all=X_train_all[arr]\n",
    "Y_train_all=Y_train_all[arr]\n",
    "\n",
    "VALIDATION_SIZE = 1000    #验证集大小\n",
    "x_val, y_val = X_train_all[:VALIDATION_SIZE], Y_train_all[:VALIDATION_SIZE]\n",
    "x_train, y_train = X_train_all[VALIDATION_SIZE:], Y_train_all[VALIDATION_SIZE:]\n",
    "print(x_train.shape, x_val.shape)\n",
    "print(y_val[0].shape)\n",
    "print(x_train[5][..., 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_augment(data):\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.Crop(percent=0.01), # # 从每侧裁剪图像0到16px（随机选择）\n",
    "        iaa.Fliplr(0.5), # 水平翻转图像 括号内为Probability of each image to get flipped.\n",
    "        iaa.Flipud(0.5), #上下翻转\n",
    "        #iaa.GaussianBlur(sigma=(0, 3.0)),  # 使用0到3.0的sigma模糊图像\n",
    "        iaa.Affine(scale=(0.7, 1.3), translate_percent=0.01, rotate=iap.Normal(-20, 20)),#旋转\n",
    "        iaa.Multiply(iap.Positive(iap.Normal(0.0, 0.1)) + 1.0),#明暗变化\n",
    "        #iaa.AddElementwise(iap.Discretize((iap.Beta(0.5, 0.5) * 2 - 1.0) * 64))\n",
    "        #iaa.AdditiveGaussianNoise(scale=(0,  0.05*255)),\n",
    "        #iaa.Sharpen(alpha=0.5),\n",
    "        #iaa.Scale((0.5, 1.5))\n",
    "    ],random_order=True)#每个batch中的Augmenters顺序不一样\n",
    "    x_batch = seq.augment_images(data)\n",
    "    return x_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#展示数据扩增后的效果\n",
    "imglist = []\n",
    "#img = imread('data/train/3006.Bmp', as_grey=True)\n",
    "sample_id = 89\n",
    "sample_x = x_train[sample_id]\n",
    "#img = cv2.imread('data/train/2212.Bmp', 0)\n",
    "print(int2lable[np.argmax(y_train[sample_id], axis=0)])\n",
    "plt.imshow(sample_x)\n",
    "plt.show()\n",
    "imglist.append(sample_x)\n",
    "for i in range(10):\n",
    "    images_aug = batch_augment(imglist)\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(images_aug[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_19:\n",
    "    def __init__(self, weights=None, sess=None):\n",
    "        self.vgg_mean = [103.939, 116.779, 123.68]\n",
    "        self.build_model()\n",
    "        self.probs = self.fc8\n",
    "        if weights is not None and sess is not None:\n",
    "            self.load_weights(weights, sess)\n",
    "    \n",
    "    def maxpool(self,name,input_data, trainable):\n",
    "        out = tf.nn.max_pool(input_data,[1,2,2,1],[1,2,2,1],padding=\"SAME\",name=name)\n",
    "        return out\n",
    "    \n",
    "    def conv(self,name, input_data, out_channel, trainable):\n",
    "        in_channel = input_data.get_shape()[-1]\n",
    "        with tf.variable_scope(name):\n",
    "            kernel = tf.get_variable(\"weights\", [3, 3, in_channel, out_channel], dtype=tf.float32,trainable=False)\n",
    "            biases = tf.get_variable(\"biases\", [out_channel], dtype=tf.float32,trainable=False)\n",
    "            conv_res = tf.nn.conv2d(input_data, kernel, [1, 1, 1, 1], padding=\"SAME\")\n",
    "            res = tf.nn.bias_add(conv_res, biases)\n",
    "            out = tf.nn.relu(res, name=name)\n",
    "        self.parameters += [kernel, biases]\n",
    "        return out\n",
    "    \n",
    "    def fc(self,name,input_data,out_channel,trainable = True):\n",
    "        shape = input_data.get_shape().as_list()\n",
    "        if len(shape) == 4:\n",
    "            size = shape[-1] * shape[-2] * shape[-3]\n",
    "        else:size = shape[1]\n",
    "        input_data_flat = tf.reshape(input_data,[-1,size])\n",
    "        with tf.variable_scope(name):\n",
    "            weights = tf.get_variable(name=\"weights\",shape=[size,out_channel],dtype=tf.float32,trainable = trainable)\n",
    "            biases = tf.get_variable(name=\"biases\",shape=[out_channel],dtype=tf.float32,trainable = trainable)\n",
    "            res = tf.matmul(input_data_flat,weights)\n",
    "            out = tf.nn.relu(tf.nn.bias_add(res,biases))\n",
    "        self.parameters += [weights, biases]\n",
    "        return out\n",
    "    \n",
    "    def build_model(self, x, classnum, is_training):\n",
    "        # Preprocess\n",
    "        # Convert RGB to BGR opencv 读取的为BGR格式，所以不用转换\n",
    "        #red, green, blue = tf.split(axis=3, num_or_size_splits=3, value=self.rgb)#RGB格式\n",
    "        bluem,  green, red = tf.split(axis=3, num_or_size_splits=3, value=x)#张量被切为三份\n",
    "        bgr = tf.concat(axis=3, values=[blue - VGG_MEAN[0], green - VGG_MEAN[1],red - VGG_MEAN[2]])\n",
    "        \n",
    "        # Block 1\n",
    "        #对于每个卷积层和全连接层中，不需要训练的权重全部被设置为trainable=False\n",
    "        self.conv1_1 = self.conv(\"conv1_1\",self.imgs,64,trainable=False)\n",
    "        self.conv1_2 = self.conv(\"conv1_2\",self.conv1_1,64,trainable=False)\n",
    "        self.pool1 = self.maxpool(\"pool1\",self.conv1_2,trainable=False)\n",
    "        \n",
    "        # Block 2\n",
    "        self.conv2_1 = self.conv(\"conv2_1\",self.pool1,128,trainable=False)\n",
    "        self.conv2_2 = self.conv(\"conv2_2\",self.conv2_1,128,trainable=False)\n",
    "        self.pool2 = self.maxpool(\"pool2\",self.conv2_2,trainable=False)  \n",
    "        \n",
    "        # Block 3\n",
    "        self.conv3_1 = self.conv(\"conv3_1\",self.pool2,256,trainable=False)\n",
    "        self.conv3_2 = self.conv(\"conv3_2\",self.conv3_1,256,trainable=False)\n",
    "        self.conv3_3 = self.conv(\"conv3_3\",self.conv3_2,256,trainable=False)\n",
    "        self.conv3_4 = self.conv(\"conv3_3\",self.conv3_3,256,trainable=False)\n",
    "        self.pool3 = self.maxpool(\"poolre3\",self.conv3_4,trainable=False)  \n",
    "        \n",
    "        # Block 4\n",
    "        self.conv4_1 = self.conv(\"conv4_1\",self.pool3,512,trainable=False)\n",
    "        self.conv4_2 = self.conv(\"conv4_2\",self.conv4_1,512,trainable=False)\n",
    "        self.conv4_3 = self.conv(\"conv4_3\",self.conv4_2,512,trainable=False)\n",
    "        self.conv4_4 = self.conv(\"conv4_3\",self.conv4_3,512,trainable=False)\n",
    "        self.pool4 = self.maxpool(\"pool4\",self.conv4_4,trainable=False)\n",
    "        \n",
    "        # Block 5\n",
    "        self.conv5_1 = self.conv(\"conv5_1\",self.pool4,512,trainable=False)\n",
    "        self.conv5_2 = self.conv(\"conv5_2\",self.conv5_1,512,trainable=False)\n",
    "        self.conv5_3 = self.conv(\"conv5_3\",self.conv5_2,512,trainable=False)\n",
    "        self.conv5_4 = self.conv(\"conv5_4\",self.conv5_3,512,trainable=False)\n",
    "        self.pool5 = self.maxpool(\"poorwel5\",self.conv5_4,trainable=False)\n",
    "        \n",
    "        # model modification for cifar-10\n",
    "        self.fc6 = self.fc(\"fc6\", self.pool5, 4096,trainable=False)\n",
    "        self.fc7 = self.fc(\"fc7\", self.fc6, 4096,trainable=False)\n",
    "        self.fc8 = self.fc(\"fc8\", self.fc7, classnum)\n",
    "    \n",
    "    def load_weights(self, weight_file, sess):\n",
    "        weights = np.load(weight_file)\n",
    "        keys = sorted(weights.keys())\n",
    "        for i, k in enumerate(keys):\n",
    "            if i not in [36,37]:#最后一层不参加训练\n",
    "                sess.run(self.parameters[i].assign(weights[k]))\n",
    "        print(\"-----------Load weights done!---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义模型 NIN\n",
    "def VGG_19(x, classnum, is_training):\n",
    "    # Block 1\n",
    "    conv1 = tf.layers.conv2d(x, 64, 3, strides=1, padding='SAME')\n",
    "    norm1 = tf.layers.batch_normalization(conv1, center=True, scale=True, training=is_training)\n",
    "    relu1 = tf.nn.relu(norm1)\n",
    "    conv2 = tf.layers.conv2d(relu1, 64, 3, strides=1, padding='SAME')\n",
    "    norm2 = tf.layers.batch_normalization(conv2, center=True, scale=True, training=is_training)\n",
    "    relu2 = tf.nn.relu(norm2)\n",
    "    pool1 = tf.layers.max_pooling2d(relu2, pool_size=[2, 2], strides=2, padding='SAME')\n",
    "    \n",
    "    # Block 2\n",
    "    conv3 = tf.layers.conv2d(pool1, 128, 3, strides=1, padding='SAME')\n",
    "    norm3 = tf.layers.batch_normalization(conv3, center=True, scale=True, training=is_training)\n",
    "    relu3 = tf.nn.relu(norm3)\n",
    "    conv4 = tf.layers.conv2d(relu3, 128, 3, strides=1, padding='SAME')\n",
    "    norm4 = tf.layers.batch_normalization(conv4, center=True, scale=True, training=is_training)\n",
    "    relu4 = tf.nn.relu(norm4)\n",
    "    pool2 = tf.layers.max_pooling2d(relu4, pool_size=[2, 2], strides=2, padding='SAME')    \n",
    "    \n",
    "    # Block 3\n",
    "    conv5 = tf.layers.conv2d(pool2, 256, 3, strides=1, padding='SAME')\n",
    "    norm5 = tf.layers.batch_normalization(conv5, center=True, scale=True, training=is_training)\n",
    "    relu5 = tf.nn.relu(norm5)\n",
    "    conv6 = tf.layers.conv2d(relu5, 256, 3, strides=1, padding='SAME')\n",
    "    norm6 = tf.layers.batch_normalization(conv6, center=True, scale=True, training=is_training)\n",
    "    relu6 = tf.nn.relu(norm6)\n",
    "    conv7 = tf.layers.conv2d(relu6, 256, 3, strides=1, padding='SAME')\n",
    "    norm7 = tf.layers.batch_normalization(conv7, center=True, scale=True, training=is_training)\n",
    "    relu7 = tf.nn.relu(norm7)\n",
    "    conv8 = tf.layers.conv2d(relu7, 256, 3, strides=1, padding='SAME')\n",
    "    norm8 = tf.layers.batch_normalization(conv8, center=True, scale=True, training=is_training)\n",
    "    relu8 = tf.nn.relu(norm8)\n",
    "    pool3 = tf.layers.max_pooling2d(relu8, pool_size=[2, 2], stride=2, padding='SAME')\n",
    "    \n",
    "    # Block 4\n",
    "    conv9 = tf.layers.conv2d(pool3, 512, 3, strides=1, padding='SAME')\n",
    "    norm9 = tf.layers.batch_normalization(conv9, center=True, scale=True, training=is_training)\n",
    "    relu9 = tf.nn.relu(norm9)\n",
    "    conv10 = tf.layers.conv2d(relu9, 512, 3, strides=1, padding='SAME')\n",
    "    norm10 = tf.layers.batch_normalization(conv10, center=True, scale=True, training=is_training)\n",
    "    relu10 = tf.nn.relu(norm10)\n",
    "    conv11 = tf.layers.conv2d(relu10, 512, 3, strides=1, padding='SAME')\n",
    "    norm11 = tf.layers.batch_normalization(conv11, center=True, scale=True, training=is_training)\n",
    "    relu11 = tf.nn.relu(norm11)\n",
    "    conv12 = tf.layers.conv2d(relu11, 512, 3, strides=1, padding='SAME')\n",
    "    norm12 = tf.layers.batch_normalization(conv12, center=True, scale=True, training=is_training)\n",
    "    relu12 = tf.nn.relu(norm12)\n",
    "    pool4 = tf.layers.max_pooling2d(relu12, pool_size=[2, 2], stride=2, padding='SAME')    \n",
    "    \n",
    "    # Block 5\n",
    "    conv13 = tf.layers.conv2d(pool4, 512, 3, strides=1, padding='SAME')\n",
    "    norm13 = tf.layers.batch_normalization(conv13, center=True, scale=True, training=is_training)\n",
    "    relu13 = tf.nn.relu(norm13)\n",
    "    conv14 = tf.layers.conv2d(relu13, 512, 3, strides=1, padding='SAME')\n",
    "    norm14 = tf.layers.batch_normalization(conv14, center=True, scale=True, training=is_training)\n",
    "    relu14 = tf.nn.relu(norm14)\n",
    "    conv15 = tf.layers.conv2d(relu14, 512, 3, strides=1, padding='SAME')\n",
    "    norm15 = tf.layers.batch_normalization(conv15, center=True, scale=True, training=is_training)\n",
    "    relu15 = tf.nn.relu(norm15)\n",
    "    conv16 = tf.layers.conv2d(relu15, 512, 3, strides=1, padding='SAME')\n",
    "    norm16 = tf.layers.batch_normalization(conv16, center=True, scale=True, training=is_training)\n",
    "    relu16 = tf.nn.relu(norm16)\n",
    "    pool5 = tf.layers.max_pooling2d(relu16, pool_size=[2, 2], stride=2, padding='SAME')\n",
    "    \n",
    "    # model modification for cifar-10\n",
    "    flatten = tf.reshape(pool4, [-1, 1*1*512])\n",
    "    fc1 = tf.layers.dense(flatten, 4096)\n",
    "    norm17 = tf.layers.batch_normalization(fc1, center=True, scale=True, training=is_training)\n",
    "    relu17 = tf.nn.relu(norm17)\n",
    "    if is_training == True:\n",
    "        relu17 = tf.layers.dropout(relu17, 0.5) \n",
    "    fc2= tf.layers.dense(relu17, 4096)\n",
    "    norm18 = tf.layers.batch_normalization(fc2, center=True, scale=True, training=is_training)\n",
    "    relu18 = tf.nn.relu(norm18)\n",
    "    if is_training == True:\n",
    "        relu18 = tf.layers.dropout(relu18, 0.5)\n",
    "    fc3 = tf.layers.dense(relu18, classnum)\n",
    "    return fc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 训练 ###\n",
    "#训练参数\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 500             #迭代次数\n",
    "EARLY_STOP_PATIENCE = 100 #控制early stopping的参数\n",
    "\n",
    "tf.reset_default_graph()\n",
    "x_data = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y_data = tf.placeholder(tf.float32, [None, 10])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "ckpt_path = './model/nin/mode.ckpt'\n",
    "\n",
    "predict = NIN(x_data, 10, is_training)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predict, labels=y_data))\n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):#批归一化层\n",
    "    train_step = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(predict, 1), tf.argmax(y_data, 1)), tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    best_validation_loss = 1000000.0\n",
    "    current_epoch = 0\n",
    "    \n",
    "    epoch = EPOCHS\n",
    "    batch_size = BATCH_SIZE\n",
    "    train_size = len(x_train)\n",
    "    train_index = list(range(train_size))\n",
    "    for n in range(epoch):\n",
    "        random.shuffle(train_index)  # 每个epoch都shuffle一下效果更好\n",
    "        x_train_, y_train_ = x_train[train_index], y_train[train_index]\n",
    "        #添加交叉验证\n",
    "        #x_train, x_val, y_train, y_val = train_test_split(train_img, train_y, test_size=0.1, random_state=42, shuffle=True)\n",
    "        for i in range(0, train_size, batch_size):\n",
    "            x_batch = x_train_[i : i + batch_size]\n",
    "            y_batch = y_train_[i : i + batch_size]\n",
    "            _, loss_step = sess.run([train_step, loss], \\\n",
    "                             feed_dict={x_data:x_batch, y_data:y_batch, is_training:True})\n",
    "            #数据扩充\n",
    "            x_batch_aug = batch_augment(x_batch)\n",
    "            _, loss_aug = sess.run([train_step, loss], \\\n",
    "                                    feed_dict={x_data:x_batch_aug, y_data:y_batch, is_training:True})\n",
    "        if n % 5 == 0:\n",
    "            #print(predict.eval(feed_dict={x_data:x_val, y_data:y_val, is_training:False}))\n",
    "            validation_loss, accuracy = sess.run([loss, acc], feed_dict={x_data:x_val, y_data:y_val, is_training:False})\n",
    "            #validation_loss = loss.eval(feed_dict={x_data:x_val, y_data:y_val, is_training:False})\n",
    "            #accuracy = acc.eval(feed_dict={x_data:x_val, y_data:y_val, is_training:False})\n",
    "            print(\"epoch %d train loss is %f validataion loss is %f accuracy is %f\" % (n, loss_step, validation_loss, accuracy))\n",
    "        if validation_loss < best_validation_loss:\n",
    "            print('----  epoch %d current best_validation_loss is %f' % (n, validation_loss))\n",
    "            best_validation_loss = validation_loss\n",
    "            current_epoch = n\n",
    "            saver.save(sess, ckpt_path)\n",
    "        elif (n - current_epoch) >= EARLY_STOP_PATIENCE:\n",
    "            print('early stoping')\n",
    "            break  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
