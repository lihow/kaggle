{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from torchnet import meter\n",
    "from progressbar import * #进度条\n",
    "from natsort import natsorted\n",
    "import torch.nn.functional as F #torch是关于运算的包\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms as T\n",
    "from torchvision import datasets,transforms, models #torchvision则是打包了一些数据集\n",
    "\n",
    "#如果多gpu运行\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='2, 3' \n",
    "#否则\n",
    "#os.environ['CUDA_VISIBLE_DEVICES']='2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConv2d(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(input_channels, output_channels, bias=False, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(output_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_Stem(nn.Module):\n",
    "    \"\"\"Figure 3. The schema for stem od the pure Inception-v4 and Inception-RedNet-v2 networks this is the input part of those\n",
    "        networks\n",
    "        if input_size = [1, 3, 32, 32],  output_size = [1, 3, 384, 384]\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            BasicConv2d(input_channels, 32, kernel_size=3),\n",
    "            BasicConv2d(32, 32,kernel_size=3, padding=1),\n",
    "            BasicConv2d(32, 64, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.branch3x3_conv = BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
    "        self.branch3x3_pool = nn.MaxPool2d(3, stride=1, padding=1)\n",
    "        self.branch7x7a = nn.Sequential(\n",
    "            BasicConv2d(160, 64, kernel_size=1),\n",
    "            BasicConv2d(64, 64, kernel_size=(7, 1), padding=(3, 0)),\n",
    "            BasicConv2d(64, 64, kernel_size=(1, 7), padding=(0, 3)),\n",
    "            BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.branch7x7b = nn.Sequential(\n",
    "            BasicConv2d(160, 64, kernel_size=1),\n",
    "            BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.branchpoola = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.branchpoolb = BasicConv2d(192, 192, kernel_size=3, stride=1, padding=1)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = [\n",
    "            self.branch3x3_conv(x),\n",
    "            self.branch3x3_pool(x)\n",
    "        ]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = [\n",
    "            self.branch7x7a(x),\n",
    "            self.branch7x7b(x)\n",
    "        ]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = [\n",
    "            self.branchpoola(x),\n",
    "            self.branchpoolb(x)\n",
    "        ]\n",
    "        x = torch.cat(x, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionA(nn.Module):\n",
    "    \"\"\"\n",
    "    Figure 4 the schema for 35x35 grid modules of the pure Inception-v4 network. This is the Inception-A bock of Figure 9\n",
    "    if input_size=[1, 384, 32, 32], the output_size=[1, 96, 32, 32]\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels):\n",
    "        super().__init__()\n",
    "        self.branch3x3stack = nn.Sequential(\n",
    "            BasicConv2d(input_channels, 64, kernel_size=1),\n",
    "            BasicConv2d(64, 96, kernel_size=3, padding=1),\n",
    "            BasicConv2d(96, 96, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.branch3x3 = nn.Sequential(\n",
    "            BasicConv2d(input_channels, 64, kernel_size=1),\n",
    "            BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.branch1x1 = BasicConv2d(input_channels, 96, kernel_size=1)\n",
    "        self.branchpool = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            BasicConv2d(input_channels, 96, kernel_size=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = [\n",
    "            self.branch3x3stack(x),\n",
    "            self.branch3x3(x),\n",
    "            self.branch1x1(x),\n",
    "            self.branchpool(x)\n",
    "        ]\n",
    "        return torch.cat(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 384, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "net = InceptionA(384)\n",
    "y = net(torch.randn(1, 384, 32, 32))\n",
    "#print(net)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReductionA(nn.Module):\n",
    "\n",
    "    #\"\"\"Figure 7. The schema for 35 × 35 to 17 × 17 reduction module. \n",
    "    #Different variants of this blocks (with various number of filters) \n",
    "    #are used in Figure 9, and 15 in each of the new Inception(-v4, - ResNet-v1,\n",
    "    #-ResNet-v2) variants presented in this paper. The k, l, m, n numbers \n",
    "    #represent filter bank sizes which can be looked up in Table 1.\n",
    "    # if input_size=[1, 96, 32, 32], k=192, l=224, m=256, n=384,the output_size=[1, 734, 16, 16]\n",
    "    def __init__(self, input_channels, k, l, m, n):\n",
    "\n",
    "        super().__init__()\n",
    "        self.branch3x3stack = nn.Sequential(\n",
    "            BasicConv2d(input_channels, k, kernel_size=1),\n",
    "            BasicConv2d(k, l, kernel_size=3, padding=1),\n",
    "            BasicConv2d(l, m, kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.branch3x3 = BasicConv2d(input_channels, n, kernel_size=3, stride=2)\n",
    "        self.branchpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.output_channels = input_channels + n + m\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = [\n",
    "            self.branch3x3stack(x),\n",
    "            self.branch3x3(x),\n",
    "            self.branchpool(x)\n",
    "        ]\n",
    "\n",
    "        return torch.cat(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionB(nn.Module):\n",
    "\n",
    "    #\"\"\"Figure 5. The schema for 17 × 17 grid modules of the pure Inception-v4 network. \n",
    "    #This is the Inception-B block of Figure 9.\"\"\"\n",
    "    # if input_size=[1, 734, 16, 16], output_size=[1, 1024, 16, 16]\n",
    "    def __init__(self, input_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.branch7x7stack = nn.Sequential(\n",
    "            BasicConv2d(input_channels, 192, kernel_size=1),\n",
    "            BasicConv2d(192, 192, kernel_size=(1, 7), padding=(0, 3)),\n",
    "            BasicConv2d(192, 224, kernel_size=(7, 1), padding=(3, 0)),\n",
    "            BasicConv2d(224, 224, kernel_size=(1, 7), padding=(0, 3)),\n",
    "            BasicConv2d(224, 256, kernel_size=(7, 1), padding=(3, 0))\n",
    "        )\n",
    "\n",
    "        self.branch7x7 = nn.Sequential(\n",
    "            BasicConv2d(input_channels, 192, kernel_size=1),\n",
    "            BasicConv2d(192, 224, kernel_size=(1, 7), padding=(0, 3)),\n",
    "            BasicConv2d(224, 256, kernel_size=(7, 1), padding=(3, 0))\n",
    "        )\n",
    "\n",
    "        self.branch1x1 = BasicConv2d(input_channels, 384, kernel_size=1) \n",
    "\n",
    "        self.branchpool = nn.Sequential(\n",
    "            nn.AvgPool2d(3, stride=1, padding=1),\n",
    "            BasicConv2d(input_channels, 128, kernel_size=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = [\n",
    "            self.branch1x1(x),\n",
    "            self.branch7x7(x),\n",
    "            self.branch7x7stack(x),\n",
    "            self.branchpool(x)\n",
    "        ]\n",
    "\n",
    "        return torch.cat(x, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReductionB(nn.Module):\n",
    "\n",
    "    #\"\"\"Figure 8. The schema for 17 × 17 to 8 × 8 grid-reduction mod- ule. \n",
    "    #This is the reduction module used by the pure Inception-v4 network in \n",
    "    #Figure 9.\"\"\"\n",
    "    # if input_size=[1, 1024, 16, 16], output_size=[1, 1536, 8, 8]\n",
    "    def __init__(self, input_channels):\n",
    "\n",
    "        super().__init__()\n",
    "        self.branch7x7 = nn.Sequential(\n",
    "            BasicConv2d(input_channels, 256, kernel_size=1),\n",
    "            BasicConv2d(256, 256, kernel_size=(1, 7), padding=(0, 3)),\n",
    "            BasicConv2d(256, 320, kernel_size=(7, 1), padding=(3, 0)),\n",
    "            BasicConv2d(320, 320, kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        self.branch3x3 = nn.Sequential(\n",
    "            BasicConv2d(input_channels, 192, kernel_size=1),\n",
    "            BasicConv2d(192, 192, kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        self.branchpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = [\n",
    "            self.branch3x3(x),\n",
    "            self.branch7x7(x),\n",
    "            self.branchpool(x)\n",
    "        ]\n",
    "\n",
    "        return torch.cat(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1536, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "net = ReductionB(1024)\n",
    "y = net(torch.randn(1, 1024, 16, 16))\n",
    "#print(net)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionC(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels):\n",
    "        #\"\"\"Figure 6. The schema for 8×8 grid modules of the pure \n",
    "        #Inceptionv4 network. This is the Inception-C block of Figure 9.\"\"\"\n",
    "        # if input_size=[1, 1536, 8, 8], output_size=[1, 1536, 8, 8]\n",
    "        super().__init__()\n",
    "\n",
    "        self.branch3x3stack = nn.Sequential(\n",
    "            BasicConv2d(input_channels, 384, kernel_size=1),\n",
    "            BasicConv2d(384, 448, kernel_size=(1, 3), padding=(0, 1)),\n",
    "            BasicConv2d(448, 512, kernel_size=(3, 1), padding=(1, 0)),\n",
    "        )\n",
    "        self.branch3x3stacka = BasicConv2d(512, 256, kernel_size=(1, 3), padding=(0, 1))\n",
    "        self.branch3x3stackb = BasicConv2d(512, 256, kernel_size=(3, 1), padding=(1, 0))\n",
    "    \n",
    "        self.branch3x3 = BasicConv2d(input_channels, 384, kernel_size=1)\n",
    "        self.branch3x3a = BasicConv2d(384, 256, kernel_size=(3, 1), padding=(1, 0))\n",
    "        self.branch3x3b = BasicConv2d(384, 256, kernel_size=(1, 3), padding=(0, 1))\n",
    "\n",
    "        self.branch1x1 = BasicConv2d(input_channels, 256, kernel_size=1)\n",
    "\n",
    "        self.branchpool = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            BasicConv2d(input_channels, 256, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch3x3stack_output = self.branch3x3stack(x)\n",
    "        branch3x3stack_output = [\n",
    "            self.branch3x3stacka(branch3x3stack_output),\n",
    "            self.branch3x3stackb(branch3x3stack_output)\n",
    "        ]\n",
    "        branch3x3stack_output = torch.cat(branch3x3stack_output, 1)\n",
    "\n",
    "        branch3x3_output = self.branch3x3(x)\n",
    "        branch3x3_output = [\n",
    "            self.branch3x3a(branch3x3_output),\n",
    "            self.branch3x3b(branch3x3_output)\n",
    "        ]\n",
    "        branch3x3_output = torch.cat(branch3x3_output, 1)\n",
    "\n",
    "        branch1x1_output = self.branch1x1(x)\n",
    "\n",
    "        branchpool = self.branchpool(x)\n",
    "\n",
    "        output = [\n",
    "            branch1x1_output,\n",
    "            branch3x3_output,\n",
    "            branch3x3stack_output,\n",
    "            branchpool\n",
    "        ]\n",
    "\n",
    "        return torch.cat(output, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionV4(nn.Module):\n",
    "\n",
    "    def __init__(self, A, B, C, k=192, l=224, m=256, n=384, class_nums=10):\n",
    "\n",
    "        super().__init__()\n",
    "        self.stem = Inception_Stem(3)\n",
    "        self.inception_a = self._generate_inception_module(384, 384, A, InceptionA)\n",
    "        self.reduction_a = ReductionA(384, k, l, m, n)\n",
    "        output_channels = self.reduction_a.output_channels\n",
    "        self.inception_b = self._generate_inception_module(output_channels, 1024, B, InceptionB)\n",
    "        self.reduction_b = ReductionB(1024)\n",
    "        self.inception_c = self._generate_inception_module(1536, 1536, C, InceptionC)\n",
    "        self.avgpool = nn.AvgPool2d(7)\n",
    "\n",
    "        #\"\"\"Dropout (keep 0.8)\"\"\"\n",
    "        self.dropout = nn.Dropout2d(1 - 0.8)\n",
    "        self.linear = nn.Linear(1536, class_nums)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.inception_a(x)\n",
    "        x = self.reduction_a(x)\n",
    "        x = self.inception_b(x)\n",
    "        x = self.reduction_b(x)\n",
    "        x = self.inception_c(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, 1536)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @staticmethod    \n",
    "    def _generate_inception_module(input_channels, output_channels, block_num, block):\n",
    "\n",
    "        layers = nn.Sequential()\n",
    "        for l in range(block_num):\n",
    "            layers.add_module(\"{}_{}\".format(block.__name__, l), block(input_channels))\n",
    "            input_channels = output_channels\n",
    "        \n",
    "        return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionResNetA(nn.Module):\n",
    "\n",
    "    #\"\"\"Figure 16. The schema for 35 × 35 grid (Inception-ResNet-A) \n",
    "    #module of the Inception-ResNet-v2 network.\"\"\"\n",
    "    def __init__(self, input_channels):\n",
    "\n",
    "        super().__init__()\n",
    "        self.branch3x3stack = nn.Sequential(\n",
    "            BasicConv2d(input_channels, 32, kernel_size=1),\n",
    "            BasicConv2d(32, 48, kernel_size=3, padding=1),\n",
    "            BasicConv2d(48, 64, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.branch3x3 = nn.Sequential(\n",
    "            BasicConv2d(input_channels, 32, kernel_size=1),\n",
    "            BasicConv2d(32, 32, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "        self.branch1x1 = BasicConv2d(input_channels, 32, kernel_size=1)\n",
    "\n",
    "        self.reduction1x1 = nn.Conv2d(128, 384, kernel_size=1)\n",
    "        self.shortcut = nn.Conv2d(input_channels, 384, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm2d(384)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        residual = [\n",
    "            self.branch1x1(x),\n",
    "            self.branch3x3(x),\n",
    "            self.branch3x3stack(x)\n",
    "        ]\n",
    "\n",
    "        residual = torch.cat(residual, 1)\n",
    "        residual = self.reduction1x1(residual)\n",
    "        shortcut = self.shortcut(x)\n",
    "\n",
    "        output = self.bn(shortcut + residual)\n",
    "        output = self.relu(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionResNetB(nn.Module):\n",
    "\n",
    "    #\"\"\"Figure 17. The schema for 17 × 17 grid (Inception-ResNet-B) module of \n",
    "    #the Inception-ResNet-v2 network.\"\"\"\n",
    "    def __init__(self, input_channels):\n",
    "\n",
    "        super().__init__()\n",
    "        self.branch7x7 = nn.Sequential(\n",
    "            BasicConv2d(input_channels, 128, kernel_size=1),\n",
    "            BasicConv2d(128, 160, kernel_size=(1, 7), padding=(0, 3)),\n",
    "            BasicConv2d(160, 192, kernel_size=(7, 1), padding=(3, 0))\n",
    "        )\n",
    "\n",
    "        self.branch1x1 = BasicConv2d(input_channels, 192, kernel_size=1)\n",
    "\n",
    "        self.reduction1x1 = nn.Conv2d(384, 1154, kernel_size=1)\n",
    "        self.shortcut = nn.Conv2d(input_channels, 1154, kernel_size=1)\n",
    "\n",
    "        self.bn = nn.BatchNorm2d(1154)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = [\n",
    "            self.branch1x1(x),\n",
    "            self.branch7x7(x)\n",
    "        ]\n",
    "\n",
    "        residual = torch.cat(residual, 1)\n",
    "\n",
    "        #\"\"\"In general we picked some scaling factors between 0.1 and 0.3 to scale the residuals \n",
    "        #before their being added to the accumulated layer activations (cf. Figure 20).\"\"\"\n",
    "        residual = self.reduction1x1(residual) * 0.1\n",
    "\n",
    "        shortcut = self.shortcut(x)\n",
    "\n",
    "        output = self.bn(residual + shortcut)\n",
    "        output = self.relu(output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionResNetC(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels):\n",
    "        \n",
    "        #Figure 19. The schema for 8×8 grid (Inception-ResNet-C)\n",
    "        #module of the Inception-ResNet-v2 network.\"\"\"\n",
    "        super().__init__()\n",
    "        self.branch3x3 = nn.Sequential(\n",
    "            BasicConv2d(input_channels, 192, kernel_size=1),\n",
    "            BasicConv2d(192, 224, kernel_size=(1, 3), padding=(0, 1)),\n",
    "            BasicConv2d(224, 256, kernel_size=(3, 1), padding=(1, 0))\n",
    "        )\n",
    "\n",
    "        self.branch1x1 = BasicConv2d(input_channels, 192, kernel_size=1)\n",
    "        self.reduction1x1 = nn.Conv2d(448, 2048, kernel_size=1)\n",
    "        self.shorcut = nn.Conv2d(input_channels, 2048, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm2d(2048)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = [\n",
    "            self.branch1x1(x),\n",
    "            self.branch3x3(x)\n",
    "        ]\n",
    "\n",
    "        residual = torch.cat(residual, 1)\n",
    "        residual = self.reduction1x1(residual) * 0.1\n",
    "\n",
    "        shorcut = self.shorcut(x)\n",
    "\n",
    "        output = self.bn(shorcut + residual)\n",
    "        output = self.relu(output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class InceptionResNetReductionA(nn.Module):\n",
    "\n",
    "    #\"\"\"Figure 7. The schema for 35 × 35 to 17 × 17 reduction module. \n",
    "    #Different variants of this blocks (with various number of filters) \n",
    "    #are used in Figure 9, and 15 in each of the new Inception(-v4, - ResNet-v1,\n",
    "    #-ResNet-v2) variants presented in this paper. The k, l, m, n numbers \n",
    "    #represent filter bank sizes which can be looked up in Table 1.\n",
    "    def __init__(self, input_channels, k, l, m, n):\n",
    "\n",
    "        super().__init__()\n",
    "        self.branch3x3stack = nn.Sequential(\n",
    "            BasicConv2d(input_channels, k, kernel_size=1),\n",
    "            BasicConv2d(k, l, kernel_size=3, padding=1),\n",
    "            BasicConv2d(l, m, kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.branch3x3 = BasicConv2d(input_channels, n, kernel_size=3, stride=2)\n",
    "        self.branchpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.output_channels = input_channels + n + m\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = [\n",
    "            self.branch3x3stack(x),\n",
    "            self.branch3x3(x),\n",
    "            self.branchpool(x)\n",
    "        ]\n",
    "\n",
    "        return torch.cat(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionResNetReductionB(nn.Module):\n",
    "\n",
    "    #\"\"\"Figure 18. The schema for 17 × 17 to 8 × 8 grid-reduction module. \n",
    "    #Reduction-B module used by the wider Inception-ResNet-v1 network in\n",
    "    #Figure 15.\"\"\"\n",
    "    #I believe it was a typo(Inception-ResNet-v1 should be Inception-ResNet-v2)\n",
    "    def __init__(self, input_channels):\n",
    "\n",
    "        super().__init__()\n",
    "        self.branchpool = nn.MaxPool2d(3, stride=2)\n",
    "\n",
    "        self.branch3x3a = nn.Sequential(\n",
    "            BasicConv2d(input_channels, 256, kernel_size=1),\n",
    "            BasicConv2d(256, 384, kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.branch3x3b = nn.Sequential(\n",
    "            BasicConv2d(input_channels, 256, kernel_size=1),\n",
    "            BasicConv2d(256, 288, kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.branch3x3stack = nn.Sequential(\n",
    "            BasicConv2d(input_channels, 256, kernel_size=1),\n",
    "            BasicConv2d(256, 288, kernel_size=3, padding=1),\n",
    "            BasicConv2d(288, 320, kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = [\n",
    "            self.branch3x3a(x),\n",
    "            self.branch3x3b(x),\n",
    "            self.branch3x3stack(x),\n",
    "            self.branchpool(x)\n",
    "        ]\n",
    "\n",
    "        for i in x:\n",
    "            print(i.shape)\n",
    "        x = torch.cat(x, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionResNetV2(nn.Module):\n",
    "\n",
    "    def __init__(self, A, B, C, k=256, l=256, m=384, n=384, class_nums=10):\n",
    "        super().__init__()\n",
    "        self.stem = Inception_Stem(3)\n",
    "        self.inception_resnet_a = self._generate_inception_module(384, 384, A, InceptionResNetA)\n",
    "        self.reduction_a = InceptionResNetReductionA(384, k, l, m, n)\n",
    "        output_channels = self.reduction_a.output_channels\n",
    "        self.inception_resnet_b = self._generate_inception_module(output_channels, 1154, B, InceptionResNetB)\n",
    "        self.reduction_b = InceptionResNetReductionB(1154)\n",
    "        self.inception_resnet_c = self._generate_inception_module(2146, 2048, C, InceptionResNetC)\n",
    "\n",
    "        #6x6 featuresize\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        #\"\"\"Dropout (keep 0.8)\"\"\"\n",
    "        self.dropout = nn.Dropout2d(1 - 0.8)\n",
    "        self.linear = nn.Linear(2048, class_nums)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.inception_resnet_a(x)\n",
    "        x = self.reduction_a(x)\n",
    "        x = self.inception_resnet_b(x)\n",
    "        x = self.reduction_b(x)\n",
    "        x = self.inception_resnet_c(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(-1, 2048)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def _generate_inception_module(input_channels, output_channels, block_num, block):\n",
    "\n",
    "        layers = nn.Sequential()\n",
    "        for l in range(block_num):\n",
    "            layers.add_module(\"{}_{}\".format(block.__name__, l), block(input_channels))\n",
    "            input_channels = output_channels\n",
    "        \n",
    "        return layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inceptionv4():\n",
    "    return InceptionV4(4, 7, 3)\n",
    "\n",
    "def inception_resnet_v2():\n",
    "    return InceptionResNetV2(5, 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2int = {'airplane':0, 'automobile':1, 'bird':2, 'cat':3, 'deer':4, 'dog':5, 'frog':6, 'horse':7, 'ship':8, 'truck':9}\n",
    "int2lable = {0:'airplane', 1:'automobile', 2:'bird', 3:'cat', 4:'deer', 5:'dog', 6:'frog', 7:'horse', 8:'ship', 9:'truck'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndValData(Dataset):\n",
    "    def __init__(self, img_path, csv_path, train=True, transforms=None):\n",
    "        '''\n",
    "        获得所有图片路径，并划分训练集、验证集\n",
    "        '''\n",
    "        self.train = train\n",
    "        files = natsorted(glob.glob(img_path + '/*'))\n",
    "        labels = pd.read_csv(csv_path).values[:, 1]\n",
    "        files_num = len(files)\n",
    "        break_point = int(0.9*files_num)\n",
    "        if self.train:\n",
    "            self.img_name = files[: break_point]\n",
    "            self.img_label = labels[: break_point]\n",
    "        else:\n",
    "            self.img_name = files[break_point: ]\n",
    "            self.img_label = labels[break_point: ]\n",
    "         \n",
    "   \n",
    "\n",
    "        #数据增强\n",
    "        if transforms is None:\n",
    "            normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            #训练集用数据增强\n",
    "            if self.train:\n",
    "                self.transforms = T.Compose([\n",
    "                    T.RandomCrop(32, padding=4),  #先四周填充0，在吧图像随机裁剪成32*32`，\n",
    "                    #T.Resize(256),\n",
    "                    #T.RandomResizedCrop(224),\n",
    "                    T.RandomHorizontalFlip(),  #图像一半的概率翻转，一半的概率不翻转\n",
    "                    #T.RandomVerticalFlip(),\n",
    "                    T.ToTensor(),\n",
    "                    normalize \n",
    "                ])\n",
    "            else:\n",
    "                self.transforms = T.Compose([\n",
    "                    T.Resize(32),\n",
    "                    #T.CenterCrop(224),#中心裁剪\n",
    "                    T.ToTensor(),\n",
    "                    normalize \n",
    "                ])\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        返回数据集中所有图片的个数\n",
    "        '''\n",
    "        return len(self.img_name)\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        返回一张图片的数据\n",
    "        '''\n",
    "        img_path = self.img_name[index]\n",
    "        img = Image.open(img_path)\n",
    "        img = self.transforms(img)\n",
    "        label = label2int[self.img_label[index]]\n",
    "        return img, label\n",
    "class TestData(Dataset):\n",
    "    def __init__(self, img_path, transforms=None):\n",
    "        files = natsorted(glob.glob(img_path + '/*'))\n",
    "        self.img_name = files\n",
    "        if transforms is None:\n",
    "            normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            self.transforms =  T.Compose([\n",
    "                T.Resize(32),\n",
    "                T.ToTensor(),\n",
    "                normalize\n",
    "            ])\n",
    "    def __len__(self):\n",
    "        return len(self.img_name)\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_name[index]\n",
    "        img = Image.open(img_path)\n",
    "        img = self.transforms(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_path = 'data/train'\n",
    "csv_path = 'data/trainLabels.csv'\n",
    "test_img_path = 'data/test'\n",
    "train_dataset = TrainAndValData(train_img_path, csv_path, train=True)\n",
    "val_dataset = TrainAndValData(train_img_path, csv_path, train=False)\n",
    "test_dataset = TestData(test_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train len is 45000\n",
      "torch.Size([3, 32, 32])\n",
      "6\n",
      "val len is 5000\n",
      "torch.Size([3, 32, 32])\n",
      "7\n",
      "test len is 300000\n",
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print('train len is %d' % len(train_dataset))\n",
    "print(train_dataset[0][0].shape)\n",
    "print(train_dataset[0][1])#打印标签\n",
    "print('val len is %d' % len(val_dataset))\n",
    "print(val_dataset[0][0].shape)\n",
    "print(val_dataset[0][1])\n",
    "print('test len is %d' % len(test_dataset))\n",
    "print(test_dataset[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数设置\n",
    "EPOCH = 135   #遍历数据集次数\n",
    "BATCH_SIZE = 80      #批处理尺寸(batch_size)\n",
    "#LR = 0.001        #学习率\n",
    "lr = 0.001\n",
    "lr_decay = 0.99995\n",
    "weight_decay = 1e-4\n",
    "model_path = 'model/inception_v4/inception_v4.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "the epoch 0, the train loss is 0.007973, the test loss is 0.007453, the test acc is 0.786293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lhw/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type InceptionV4. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/lhw/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Inception_Stem. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/lhw/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type BasicConv2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/lhw/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type InceptionA. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/lhw/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type ReductionA. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/lhw/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type InceptionB. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/lhw/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type ReductionB. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/lhw/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type InceptionC. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the epoch 1, the train loss is 0.007327, the test loss is 0.006595, the test acc is 0.805846\n",
      "the epoch 2, the train loss is 0.006742, the test loss is 0.006394, the test acc is 0.816512\n",
      "the epoch 3, the train loss is 0.006251, the test loss is 0.006097, the test acc is 0.820265\n",
      "the epoch 4, the train loss is 0.005776, the test loss is 0.005837, the test acc is 0.834683\n",
      "the epoch 5, the train loss is 0.005406, the test loss is 0.005922, the test acc is 0.825400\n",
      "the epoch 6, the train loss is 0.004984, the test loss is 0.006264, the test acc is 0.825005\n",
      "the epoch 7, the train loss is 0.004698, the test loss is 0.005711, the test acc is 0.834881\n",
      "the epoch 8, the train loss is 0.004382, the test loss is 0.005414, the test acc is 0.847126\n",
      "the epoch 9, the train loss is 0.004047, the test loss is 0.005264, the test acc is 0.848509\n",
      "the epoch 10, the train loss is 0.003864, the test loss is 0.005299, the test acc is 0.848509\n",
      "the epoch 11, the train loss is 0.003580, the test loss is 0.005273, the test acc is 0.850484\n",
      "the epoch 12, the train loss is 0.003329, the test loss is 0.005084, the test acc is 0.858187\n",
      "the epoch 13, the train loss is 0.003167, the test loss is 0.004687, the test acc is 0.867667\n",
      "the epoch 14, the train loss is 0.002993, the test loss is 0.005165, the test acc is 0.859964\n",
      "the epoch 15, the train loss is 0.002850, the test loss is 0.005150, the test acc is 0.861347\n",
      "the epoch 16, the train loss is 0.002661, the test loss is 0.005017, the test acc is 0.867075\n",
      "the epoch 17, the train loss is 0.002502, the test loss is 0.005068, the test acc is 0.859964\n",
      "the epoch 18, the train loss is 0.002355, the test loss is 0.004858, the test acc is 0.868852\n",
      "the epoch 19, the train loss is 0.002207, the test loss is 0.005972, the test acc is 0.852459\n",
      "the epoch 20, the train loss is 0.002041, the test loss is 0.005579, the test acc is 0.862532\n",
      "the epoch 21, the train loss is 0.001925, the test loss is 0.005208, the test acc is 0.869445\n",
      "the epoch 22, the train loss is 0.001872, the test loss is 0.005547, the test acc is 0.860359\n",
      "the epoch 23, the train loss is 0.001738, the test loss is 0.005107, the test acc is 0.873593\n",
      "the epoch 24, the train loss is 0.001667, the test loss is 0.005239, the test acc is 0.871025\n",
      "the epoch 25, the train loss is 0.001636, the test loss is 0.005674, the test acc is 0.862927\n",
      "the epoch 26, the train loss is 0.001470, the test loss is 0.005053, the test acc is 0.879321\n",
      "the epoch 27, the train loss is 0.001447, the test loss is 0.004983, the test acc is 0.877740\n",
      "the epoch 28, the train loss is 0.001353, the test loss is 0.005030, the test acc is 0.878531\n",
      "the epoch 29, the train loss is 0.001306, the test loss is 0.005314, the test acc is 0.876358\n",
      "the epoch 30, the train loss is 0.001223, the test loss is 0.005604, the test acc is 0.869643\n",
      "the epoch 31, the train loss is 0.001214, the test loss is 0.005449, the test acc is 0.874778\n",
      "the epoch 32, the train loss is 0.001078, the test loss is 0.005079, the test acc is 0.878531\n",
      "the epoch 33, the train loss is 0.001079, the test loss is 0.005508, the test acc is 0.874975\n",
      "the epoch 34, the train loss is 0.001065, the test loss is 0.005468, the test acc is 0.878926\n",
      "the epoch 35, the train loss is 0.000980, the test loss is 0.005458, the test acc is 0.870828\n",
      "the epoch 36, the train loss is 0.000974, the test loss is 0.005469, the test acc is 0.874383\n",
      "the epoch 37, the train loss is 0.000845, the test loss is 0.005241, the test acc is 0.882481\n",
      "the epoch 38, the train loss is 0.000874, the test loss is 0.005835, the test acc is 0.870235\n",
      "the epoch 39, the train loss is 0.000858, the test loss is 0.005181, the test acc is 0.883468\n",
      "the epoch 40, the train loss is 0.000821, the test loss is 0.005217, the test acc is 0.877345\n",
      "the epoch 41, the train loss is 0.000823, the test loss is 0.005683, the test acc is 0.874778\n",
      "the epoch 42, the train loss is 0.000785, the test loss is 0.005097, the test acc is 0.885838\n",
      "the epoch 43, the train loss is 0.000762, the test loss is 0.005183, the test acc is 0.879716\n",
      "the epoch 44, the train loss is 0.000722, the test loss is 0.005256, the test acc is 0.886431\n",
      "the epoch 45, the train loss is 0.000707, the test loss is 0.005103, the test acc is 0.885048\n",
      "the epoch 46, the train loss is 0.000683, the test loss is 0.005015, the test acc is 0.888604\n",
      "the epoch 47, the train loss is 0.000625, the test loss is 0.005748, the test acc is 0.875568\n",
      "the epoch 48, the train loss is 0.000630, the test loss is 0.005169, the test acc is 0.888604\n",
      "the epoch 49, the train loss is 0.000602, the test loss is 0.005195, the test acc is 0.885246\n",
      "the epoch 50, the train loss is 0.000566, the test loss is 0.005887, the test acc is 0.879716\n",
      "the epoch 51, the train loss is 0.000659, the test loss is 0.005092, the test acc is 0.886628\n",
      "the epoch 52, the train loss is 0.000583, the test loss is 0.005276, the test acc is 0.881691\n",
      "the epoch 53, the train loss is 0.000535, the test loss is 0.005623, the test acc is 0.881493\n",
      "the epoch 54, the train loss is 0.000504, the test loss is 0.005453, the test acc is 0.880703\n",
      "the epoch 55, the train loss is 0.000508, the test loss is 0.005411, the test acc is 0.886826\n",
      "the epoch 56, the train loss is 0.000502, the test loss is 0.005823, the test acc is 0.881691\n",
      "the epoch 57, the train loss is 0.000563, the test loss is 0.005029, the test acc is 0.888209\n",
      "the epoch 58, the train loss is 0.000459, the test loss is 0.005443, the test acc is 0.885838\n",
      "the epoch 59, the train loss is 0.000487, the test loss is 0.005375, the test acc is 0.883468\n",
      "the epoch 60, the train loss is 0.000460, the test loss is 0.005567, the test acc is 0.882678\n",
      "the epoch 61, the train loss is 0.000465, the test loss is 0.005660, the test acc is 0.879518\n",
      "the epoch 62, the train loss is 0.000476, the test loss is 0.005466, the test acc is 0.888801\n",
      "the epoch 63, the train loss is 0.000469, the test loss is 0.005301, the test acc is 0.886233\n",
      "the epoch 64, the train loss is 0.000450, the test loss is 0.005663, the test acc is 0.882086\n",
      "the epoch 65, the train loss is 0.000418, the test loss is 0.005429, the test acc is 0.886826\n",
      "the epoch 66, the train loss is 0.000449, the test loss is 0.005470, the test acc is 0.885443\n",
      "the epoch 67, the train loss is 0.000411, the test loss is 0.005476, the test acc is 0.886826\n",
      "the epoch 68, the train loss is 0.000445, the test loss is 0.006226, the test acc is 0.877148\n",
      "the epoch 69, the train loss is 0.000387, the test loss is 0.006014, the test acc is 0.884258\n",
      "the epoch 70, the train loss is 0.000408, the test loss is 0.005705, the test acc is 0.881691\n",
      "the epoch 71, the train loss is 0.000453, the test loss is 0.005765, the test acc is 0.888406\n",
      "the epoch 72, the train loss is 0.000407, the test loss is 0.005420, the test acc is 0.886036\n",
      "the epoch 73, the train loss is 0.000386, the test loss is 0.005314, the test acc is 0.888011\n",
      "the epoch 74, the train loss is 0.000377, the test loss is 0.005468, the test acc is 0.886628\n",
      "the epoch 75, the train loss is 0.000396, the test loss is 0.005512, the test acc is 0.888801\n",
      "the epoch 76, the train loss is 0.000372, the test loss is 0.005693, the test acc is 0.885838\n",
      "the epoch 77, the train loss is 0.000339, the test loss is 0.006255, the test acc is 0.877938\n",
      "the epoch 78, the train loss is 0.000395, the test loss is 0.005457, the test acc is 0.887616\n",
      "the epoch 79, the train loss is 0.000372, the test loss is 0.005350, the test acc is 0.890184\n",
      "the epoch 80, the train loss is 0.000338, the test loss is 0.005497, the test acc is 0.886826\n",
      "the epoch 81, the train loss is 0.000376, the test loss is 0.005756, the test acc is 0.881888\n",
      "the epoch 82, the train loss is 0.000407, the test loss is 0.005728, the test acc is 0.879518\n",
      "the epoch 83, the train loss is 0.000377, the test loss is 0.005345, the test acc is 0.887221\n",
      "the epoch 84, the train loss is 0.000316, the test loss is 0.005434, the test acc is 0.890184\n",
      "the epoch 85, the train loss is 0.000375, the test loss is 0.005872, the test acc is 0.883073\n",
      "the epoch 86, the train loss is 0.000329, the test loss is 0.005193, the test acc is 0.890974\n",
      "the epoch 87, the train loss is 0.000308, the test loss is 0.005421, the test acc is 0.889394\n",
      "the epoch 88, the train loss is 0.000345, the test loss is 0.005739, the test acc is 0.879321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the epoch 89, the train loss is 0.000353, the test loss is 0.005976, the test acc is 0.879716\n",
      "the epoch 90, the train loss is 0.000294, the test loss is 0.005634, the test acc is 0.879518\n",
      "the epoch 91, the train loss is 0.000343, the test loss is 0.005773, the test acc is 0.885246\n",
      "the epoch 92, the train loss is 0.000317, the test loss is 0.005280, the test acc is 0.892159\n",
      "the epoch 93, the train loss is 0.000288, the test loss is 0.005675, the test acc is 0.888999\n",
      "the epoch 94, the train loss is 0.000327, the test loss is 0.005422, the test acc is 0.891369\n",
      "the epoch 95, the train loss is 0.000311, the test loss is 0.005514, the test acc is 0.883666\n",
      "the epoch 96, the train loss is 0.000313, the test loss is 0.005283, the test acc is 0.886628\n",
      "the epoch 97, the train loss is 0.000295, the test loss is 0.005833, the test acc is 0.886628\n",
      "the epoch 98, the train loss is 0.000316, the test loss is 0.005479, the test acc is 0.890579\n",
      "the epoch 99, the train loss is 0.000278, the test loss is 0.005376, the test acc is 0.891566\n",
      "the epoch 100, the train loss is 0.000307, the test loss is 0.005915, the test acc is 0.882481\n",
      "the epoch 101, the train loss is 0.000260, the test loss is 0.005730, the test acc is 0.892751\n",
      "the epoch 102, the train loss is 0.000236, the test loss is 0.005459, the test acc is 0.885048\n",
      "the epoch 103, the train loss is 0.000311, the test loss is 0.005444, the test acc is 0.895912\n",
      "the epoch 104, the train loss is 0.000277, the test loss is 0.005393, the test acc is 0.889196\n",
      "the epoch 105, the train loss is 0.000291, the test loss is 0.005277, the test acc is 0.889789\n",
      "the epoch 106, the train loss is 0.000349, the test loss is 0.005312, the test acc is 0.892356\n",
      "the epoch 107, the train loss is 0.000298, the test loss is 0.005433, the test acc is 0.892949\n",
      "the epoch 108, the train loss is 0.000265, the test loss is 0.005636, the test acc is 0.889789\n",
      "the epoch 109, the train loss is 0.000284, the test loss is 0.005216, the test acc is 0.895319\n",
      "the epoch 110, the train loss is 0.000260, the test loss is 0.005639, the test acc is 0.891171\n",
      "the epoch 111, the train loss is 0.000307, the test loss is 0.005555, the test acc is 0.891171\n",
      "the epoch 112, the train loss is 0.000313, the test loss is 0.005000, the test acc is 0.899072\n",
      "the epoch 113, the train loss is 0.000240, the test loss is 0.005598, the test acc is 0.894134\n",
      "the epoch 114, the train loss is 0.000287, the test loss is 0.005183, the test acc is 0.890381\n",
      "the epoch 115, the train loss is 0.000302, the test loss is 0.005830, the test acc is 0.883863\n",
      "the epoch 116, the train loss is 0.000290, the test loss is 0.005953, the test acc is 0.888604\n",
      "the epoch 117, the train loss is 0.000282, the test loss is 0.005201, the test acc is 0.894726\n",
      "the epoch 118, the train loss is 0.000276, the test loss is 0.005068, the test acc is 0.890974\n",
      "the epoch 119, the train loss is 0.000279, the test loss is 0.005616, the test acc is 0.884456\n",
      "the epoch 120, the train loss is 0.000308, the test loss is 0.005031, the test acc is 0.897097\n",
      "the epoch 121, the train loss is 0.000265, the test loss is 0.005036, the test acc is 0.893541\n",
      "the epoch 122, the train loss is 0.000293, the test loss is 0.005498, the test acc is 0.890381\n",
      "the epoch 123, the train loss is 0.000282, the test loss is 0.005276, the test acc is 0.895516\n",
      "the epoch 124, the train loss is 0.000306, the test loss is 0.005104, the test acc is 0.897887\n",
      "the epoch 125, the train loss is 0.000265, the test loss is 0.005304, the test acc is 0.889591\n",
      "the epoch 126, the train loss is 0.000297, the test loss is 0.005596, the test acc is 0.884653\n",
      "the epoch 127, the train loss is 0.000246, the test loss is 0.005260, the test acc is 0.892159\n",
      "the epoch 128, the train loss is 0.000293, the test loss is 0.005258, the test acc is 0.892554\n",
      "the epoch 129, the train loss is 0.000252, the test loss is 0.005381, the test acc is 0.891171\n",
      "the epoch 130, the train loss is 0.000270, the test loss is 0.005034, the test acc is 0.894726\n",
      "the epoch 131, the train loss is 0.000276, the test loss is 0.004860, the test acc is 0.895319\n",
      "the epoch 132, the train loss is 0.000295, the test loss is 0.005227, the test acc is 0.892356\n",
      "the epoch 133, the train loss is 0.000246, the test loss is 0.005532, the test acc is 0.890776\n",
      "early stop\n",
      "Done Training!\n"
     ]
    }
   ],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "valloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8) \n",
    "\n",
    "net = inceptionv4()\n",
    "\n",
    "##单GPU\n",
    "#net = net.cuda()\n",
    "##多GPU\n",
    "if torch.cuda.device_count() > 1:\n",
    "    net = nn.DataParallel(net, device_ids=[0,1])\n",
    "#接着上一次训练\n",
    "if os.path.exists(model_path):\n",
    "    net = torch.load(model_path)\n",
    "net = net.cuda()\n",
    "# 定义损失函数和优化方式\n",
    "criterion = nn.CrossEntropyLoss()  #损失函数为交叉熵，多用于多分类问题\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4) #优化方式为mini-batch momentum-SGD，并采用L2正则化（权重衰减）\n",
    "#optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "#保存模型判断条件\n",
    "max_val_acc = 0\n",
    "pre_epoch = 0\n",
    "max_interval_epoch = 20\n",
    "pre_train_loss = 100000\n",
    "\n",
    "print(\"Start Training...\")\n",
    "for epoch in range(400):\n",
    "    #训练集\n",
    "    train_loss = 0\n",
    "    train_count = 0\n",
    "    net.train()\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "        #inputs, labels = inputs.to(device), labels.to(device) # 注意需要复制到GPU\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #更新指标\n",
    "        train_count += labels.size(0)\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= train_count\n",
    "\n",
    "    #验证集\n",
    "    val_acc = 0\n",
    "    val_loss = 0\n",
    "    val_count = 0\n",
    "    net.eval()\n",
    "    for i, data in enumerate(valloader):\n",
    "        inputs, labels = data\n",
    "        #inputs, labels = inputs.to(device), labels.to(device) # 注意需要复制到GPU\n",
    "        inputs, labels =  inputs.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        #更新指标\n",
    "        val_count += 1\n",
    "        val_loss += loss.item()\n",
    "        _, predict = outputs.max(1)\n",
    "        val_count += labels.size(0)\n",
    "        val_acc += (predict == labels).sum().item()\n",
    "    val_acc /= val_count\n",
    "    val_loss /= val_count\n",
    "    # print the loss and accuracy\n",
    "    print('the epoch %d, the train loss is %f, the test loss is %f, the test acc is %f' % (epoch, train_loss, val_loss, val_acc))\n",
    "\n",
    "    #保存模型\n",
    "    if val_acc > max_val_acc:\n",
    "        max_val_acc = val_acc\n",
    "        pre_epoch = epoch\n",
    "        torch.save(net, model_path)#保存整个神经网络的的结构信息和模型参数信息，save的对象是网络net\n",
    "    if epoch - pre_epoch > max_interval_epoch:\n",
    "        print('early stop')\n",
    "        break\n",
    "\n",
    "    #如果损失不载下降，则降低学习率\n",
    "    if train_loss > pre_train_loss:\n",
    "        lr = lr*lr_decay\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    pre_train_loss = pre_train_loss\n",
    "print(\"Done Training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
