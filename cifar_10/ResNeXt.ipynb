{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from torchnet import meter\n",
    "from progressbar import * #进度条\n",
    "from natsort import natsorted\n",
    "import torch.nn.functional as F #torch是关于运算的包\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms as T\n",
    "from torchvision import datasets,transforms, models #torchvision则是打包了一些数据集\n",
    "\n",
    "#如果多gpu运行，屏蔽下一句\n",
    "#os.environ['CUDA_VISIBLE_DEVICES']='3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, inplanes, planes, caridinality, base_witdth, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        D = int(planes*(base_witdth/64.))\n",
    "        C = caridinality\n",
    "        self.conv1 = nn.Conv2d(inplanes, D*C, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(D*C)\n",
    "        self.conv2 = nn.Conv2d(D*C, D*C, kernel_size=3, stride=stride, padding=1, groups=C, bias=False)\n",
    "        #groups: 控制输入和输出之间的连接，  group=1，输出是所有的输入的卷积；group=2，\n",
    "        #此时相当于有并排的两个卷积层，每个卷积层计算输入通道的一半，并且产生的输出是输出通道的一半，随后将这两个输出连接起来。\n",
    "        self.bn2 = nn.BatchNorm2d(D*C)\n",
    "        self.conv3 = nn.Conv2d(D*C, planes*4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes*4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride= stride\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        if residual.size() != out.size():\n",
    "            print(out.size(), residual.size())\n",
    "            \n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNeXt(nn.Module):\n",
    "    def __init__(self, block, layers, cardinality, base_width, num_classes=10):\n",
    "        super(ResNeXt, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.cardinality = cardinality\n",
    "        self.base_width = base_width\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(8, stride=1)\n",
    "        self.fc = nn.Linear(256*block.expansion, num_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0]*m.kernel_size[0]*m.kernel_size[1]*m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2./n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes*block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes*block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes*block.expansion)\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, self.cardinality, self.base_width, stride, downsample))\n",
    "        self.inplanes = planes*block.expansion\n",
    "        #print(blocks)\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, self.cardinality, self.base_width))\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNeXt_cifar(depth, cardinality, base_width, **kwargs):\n",
    "    assert(depth-2)%9 == 0 # 2+3*n*3 #in+out+resnet_block\n",
    "    n = int((depth-2)/9)\n",
    "    model = ResNeXt(Bottleneck, [n, n, n], cardinality, base_width, **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "net = ResNeXt_cifar(29, 16, 64)\n",
    "y = net(torch.randn(1, 3, 32, 32))\n",
    "#print(net)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2int = {'airplane':0, 'automobile':1, 'bird':2, 'cat':3, 'deer':4, 'dog':5, 'frog':6, 'horse':7, 'ship':8, 'truck':9}\n",
    "int2lable = {0:'airplane', 1:'automobile', 2:'bird', 3:'cat', 4:'deer', 5:'dog', 6:'frog', 7:'horse', 8:'ship', 9:'truck'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndValData(Dataset):\n",
    "    def __init__(self, img_path, csv_path, train=True, transforms=None):\n",
    "        '''\n",
    "        获得所有图片路径，并划分训练集、验证集\n",
    "        '''\n",
    "        self.train = train\n",
    "        files = natsorted(glob.glob(img_path + '/*'))\n",
    "        labels = pd.read_csv(csv_path).values[:, 1]\n",
    "        files_num = len(files)\n",
    "        break_point = int(0.9*files_num)\n",
    "        if self.train:\n",
    "            self.img_name = files[: break_point]\n",
    "            self.img_label = labels[: break_point]\n",
    "        else:\n",
    "            self.img_name = files[break_point: ]\n",
    "            self.img_label = labels[break_point: ]\n",
    "         \n",
    "   \n",
    "\n",
    "        #数据增强\n",
    "        if transforms is None:\n",
    "            normalize = T.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\n",
    "            #训练集用数据增强\n",
    "            if self.train:\n",
    "                self.transforms = T.Compose([\n",
    "                    #T.RandomCrop(224, padding=4),  #先四周填充0，在吧图像随机裁剪成32*32`，\n",
    "                    T.Resize(40),\n",
    "                    T.RandomResizedCrop(32),\n",
    "                    T.RandomHorizontalFlip(),  #图像一半的概率翻转，一半的概率不翻转\n",
    "                    #T.RandomVerticalFlip(),\n",
    "                    T.ToTensor(),\n",
    "                    normalize \n",
    "                ])\n",
    "            else:\n",
    "                self.transforms = T.Compose([\n",
    "                    T.Resize(32),\n",
    "                    #T.CenterCrop(224),#中心裁剪\n",
    "                    T.ToTensor(),\n",
    "                    normalize \n",
    "                ])\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        返回数据集中所有图片的个数\n",
    "        '''\n",
    "        return len(self.img_name)\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        返回一张图片的数据\n",
    "        '''\n",
    "        img_path = self.img_name[index]\n",
    "        img = Image.open(img_path)\n",
    "        img = self.transforms(img)\n",
    "        label = label2int[self.img_label[index]]\n",
    "        return img, label\n",
    "class TestData(Dataset):\n",
    "    def __init__(self, img_path, transforms=None):\n",
    "        files = natsorted(glob.glob(img_path + '/*'))\n",
    "        self.img_name = files\n",
    "        if transforms is None:\n",
    "            normalize = T.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])\n",
    "            self.transforms =  T.Compose([\n",
    "                T.Resize(32),\n",
    "                T.ToTensor(),\n",
    "                normalize\n",
    "            ])\n",
    "    def __len__(self):\n",
    "        return len(self.img_name)\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_name[index]\n",
    "        img = Image.open(img_path)\n",
    "        img = self.transforms(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_path = 'data/train'\n",
    "csv_path = 'data/trainLabels.csv'\n",
    "test_img_path = 'data/test'\n",
    "train_dataset = TrainAndValData(train_img_path, csv_path, train=True)\n",
    "val_dataset = TrainAndValData(train_img_path, csv_path, train=False)\n",
    "test_dataset = TestData(test_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train len is 45000\n",
      "torch.Size([3, 32, 32])\n",
      "6\n",
      "val len is 5000\n",
      "torch.Size([3, 32, 32])\n",
      "7\n",
      "test len is 300000\n",
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print('train len is %d' % len(train_dataset))\n",
    "print(train_dataset[0][0].shape)\n",
    "print(train_dataset[0][1])#打印标签\n",
    "print('val len is %d' % len(val_dataset))\n",
    "print(val_dataset[0][0].shape)\n",
    "print(val_dataset[0][1])\n",
    "print('test len is %d' % len(test_dataset))\n",
    "print(test_dataset[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数设置\n",
    "EPOCH = 135   #遍历数据集次数\n",
    "BATCH_SIZE = 60      #批处理尺寸(batch_size)\n",
    "#LR = 0.001        #学习率\n",
    "lr = 0.001\n",
    "lr_decay = 0.995\n",
    "weight_decay = 1e-4\n",
    "model_path = 'model/resnext/resnext.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "the epoch 0, the train loss is 0.007837, the test loss is 0.008967, the test acc is 0.832809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lhw/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type ResNeXt. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/lhw/anaconda3/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Bottleneck. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the epoch 1, the train loss is 0.007797, the test loss is 0.007933, the test acc is 0.837530\n",
      "the epoch 2, the train loss is 0.007640, the test loss is 0.007750, the test acc is 0.843430\n",
      "the epoch 3, the train loss is 0.007587, the test loss is 0.008761, the test acc is 0.825138\n",
      "the epoch 4, the train loss is 0.007468, the test loss is 0.007483, the test acc is 0.844414\n",
      "the epoch 5, the train loss is 0.007410, the test loss is 0.007053, the test acc is 0.860936\n",
      "the epoch 6, the train loss is 0.007315, the test loss is 0.009257, the test acc is 0.834776\n",
      "the epoch 7, the train loss is 0.007252, the test loss is 0.010481, the test acc is 0.810779\n",
      "the epoch 8, the train loss is 0.007120, the test loss is 0.008645, the test acc is 0.837726\n",
      "the epoch 9, the train loss is 0.007022, the test loss is 0.008860, the test acc is 0.832612\n",
      "the epoch 10, the train loss is 0.006855, the test loss is 0.007267, the test acc is 0.854249\n",
      "the epoch 11, the train loss is 0.006852, the test loss is 0.007418, the test acc is 0.847758\n",
      "the epoch 12, the train loss is 0.006731, the test loss is 0.008591, the test acc is 0.839693\n",
      "the epoch 13, the train loss is 0.006546, the test loss is 0.006333, the test acc is 0.872935\n",
      "the epoch 14, the train loss is 0.006662, the test loss is 0.007936, the test acc is 0.853068\n",
      "the epoch 15, the train loss is 0.006532, the test loss is 0.006476, the test acc is 0.860740\n",
      "the epoch 16, the train loss is 0.006537, the test loss is 0.009141, the test acc is 0.830842\n",
      "the epoch 17, the train loss is 0.006519, the test loss is 0.007215, the test acc is 0.856216\n",
      "the epoch 18, the train loss is 0.006330, the test loss is 0.007206, the test acc is 0.856412\n",
      "the epoch 19, the train loss is 0.006352, the test loss is 0.008222, the test acc is 0.842447\n",
      "the epoch 20, the train loss is 0.006268, the test loss is 0.007949, the test acc is 0.846774\n",
      "the epoch 21, the train loss is 0.006186, the test loss is 0.007765, the test acc is 0.854642\n",
      "the epoch 22, the train loss is 0.006059, the test loss is 0.007141, the test acc is 0.863690\n",
      "the epoch 23, the train loss is 0.006069, the test loss is 0.007318, the test acc is 0.859756\n",
      "the epoch 24, the train loss is 0.006184, the test loss is 0.007076, the test acc is 0.859756\n",
      "the epoch 25, the train loss is 0.005993, the test loss is 0.006978, the test acc is 0.862116\n",
      "the epoch 26, the train loss is 0.005971, the test loss is 0.006912, the test acc is 0.866640\n",
      "the epoch 27, the train loss is 0.005818, the test loss is 0.006802, the test acc is 0.863493\n",
      "the epoch 28, the train loss is 0.005831, the test loss is 0.006623, the test acc is 0.869591\n",
      "the epoch 29, the train loss is 0.005768, the test loss is 0.007558, the test acc is 0.857986\n",
      "the epoch 30, the train loss is 0.005782, the test loss is 0.007371, the test acc is 0.859166\n",
      "the epoch 31, the train loss is 0.005720, the test loss is 0.007292, the test acc is 0.867427\n",
      "the epoch 32, the train loss is 0.005655, the test loss is 0.007405, the test acc is 0.855429\n",
      "the epoch 33, the train loss is 0.005659, the test loss is 0.006575, the test acc is 0.869788\n",
      "the epoch 34, the train loss is 0.005632, the test loss is 0.008238, the test acc is 0.853068\n",
      "early stop\n",
      "Done Training!\n"
     ]
    }
   ],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "valloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8) \n",
    "\n",
    "net = ResNeXt_cifar(29, 16, 64)\n",
    "\n",
    "##单GPU\n",
    "#net = net.cuda()\n",
    "##多GPU\n",
    "if torch.cuda.device_count() > 1:\n",
    "    net = nn.DataParallel(net, device_ids=[0,1])\n",
    "#接着上一次训练\n",
    "if os.path.exists(model_path):\n",
    "    net = torch.load(model_path)\n",
    "net = net.cuda()\n",
    "# 定义损失函数和优化方式\n",
    "criterion = nn.CrossEntropyLoss()  #损失函数为交叉熵，多用于多分类问题\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4) #优化方式为mini-batch momentum-SGD，并采用L2正则化（权重衰减）\n",
    "#optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "#保存模型判断条件\n",
    "max_val_acc = 0\n",
    "pre_epoch = 0\n",
    "max_interval_epoch = 20\n",
    "pre_train_loss = 100000\n",
    "\n",
    "print(\"Start Training...\")\n",
    "for epoch in range(400):\n",
    "    #训练集\n",
    "    train_loss = 0\n",
    "    train_count = 0\n",
    "    net.train()\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "        #inputs, labels = inputs.to(device), labels.to(device) # 注意需要复制到GPU\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #更新指标\n",
    "        train_count += labels.size(0)\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= train_count\n",
    "\n",
    "    #验证集\n",
    "    val_acc = 0\n",
    "    val_loss = 0\n",
    "    val_count = 0\n",
    "    net.eval()\n",
    "    for i, data in enumerate(valloader):\n",
    "        inputs, labels = data\n",
    "        #inputs, labels = inputs.to(device), labels.to(device) # 注意需要复制到GPU\n",
    "        inputs, labels =  inputs.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        #更新指标\n",
    "        val_count += 1\n",
    "        val_loss += loss.item()\n",
    "        _, predict = outputs.max(1)\n",
    "        val_count += labels.size(0)\n",
    "        val_acc += (predict == labels).sum().item()\n",
    "    val_acc /= val_count\n",
    "    val_loss /= val_count\n",
    "    # print the loss and accuracy\n",
    "    print('the epoch %d, the train loss is %f, the test loss is %f, the test acc is %f' % (epoch, train_loss, val_loss, val_acc))\n",
    "\n",
    "    #保存模型\n",
    "    if val_acc > max_val_acc:\n",
    "        max_val_acc = val_acc\n",
    "        pre_epoch = epoch\n",
    "        torch.save(net, model_path)#保存整个神经网络的的结构信息和模型参数信息，save的对象是网络net\n",
    "    if epoch - pre_epoch > max_interval_epoch:\n",
    "        print('early stop')\n",
    "        break\n",
    "\n",
    "    #如果损失不载下降，则降低学习率\n",
    "    if train_loss > pre_train_loss:\n",
    "        lr = lr*lr_decay\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    pre_train_loss = pre_train_loss\n",
    "print(\"Done Training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit(model_path, test_dataset, output_file):\n",
    "    net = ResNeXt_cifar(29, 16, 64)\n",
    "\n",
    "    ##单GPU\n",
    "    #net = net.cuda()\n",
    "    ##多GPU\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        net = nn.DataParallel(net, device_ids=[0,1])\n",
    "    #加载模型\n",
    "    net = torch.load(model_path)#加载整个网络和参数\n",
    "    #device = torch.device(\"cuda:2\")\n",
    "    #net = net.to(device)\n",
    "    net = net.cuda()\n",
    "    net.eval()\n",
    "    #数据\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=60, shuffle=False, num_workers=4)\n",
    "    total = len(test_dataset)\n",
    "    \n",
    "    f = open(output_file, 'w')\n",
    "    f.write('id,label\\n')\n",
    "    test_count = 0\n",
    "    pbar = ProgressBar().start()\n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        pbar.update(int((test_count / (total - 1)) * 100))#进度条\n",
    "        time.sleep(0.01) \n",
    "        \n",
    "        inputs = data\n",
    "        #inputs = inputs.to(device)\n",
    "        inputs = inputs.cuda()\n",
    "        outputs = net(inputs)\n",
    "        outputs = outputs.cuda()\n",
    "        #_, predict = outputs.max(1)\n",
    "        #print([predict[0].data.cpu().numpy()][0].shape)\n",
    "        for j in range(len(outputs)):\n",
    "            f.write(\"\".join([str(test_count+j+1), ',', int2lable[outputs[j].data.cpu().numpy().argmax()], '\\n']))\n",
    "        test_count += len(outputs)\n",
    "    f.close()\n",
    "    pbar.finish()\n",
    "    print('Finish!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_file = \"submission.csv\"\n",
    "submit(model_path, test_dataset, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
