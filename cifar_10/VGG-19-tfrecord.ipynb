{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lhw/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/lhw/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import imgaug as ia\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from progressbar import * #进度条\n",
    "from natsort import natsorted\n",
    "import matplotlib.pyplot as plt\n",
    "from imgaug import augmenters as iaa\n",
    "from imgaug import parameters as iap\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread, imsave\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from skimage import transform, filters, exposure\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='3'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2int = {'airplane':0, 'automobile':1, 'bird':2, 'cat':3, 'deer':4, 'dog':5, 'frog':6, 'horse':7, 'ship':8, 'truck':9}\n",
    "int2lable = {0:'airplane', 1:'automobile', 2:'bird', 3:'cat', 4:'deer', 5:'dog', 6:'frog', 7:'horse', 8:'ship', 9:'truck'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#由于数据集较大，采用tensorflow自带的TFRecord进行读取\n",
    "def Trainset2TFRecord2(train_tfrecords_path, val_tfrecords_path, ratio=0.9, data_path='data'):\n",
    "    files = natsorted(glob.glob(data_path + '/train/*'))\n",
    "    total = len(files)\n",
    "    train_num = int(total*ratio)\n",
    "    val_num = total-train_num\n",
    "    if (os.path.exists(train_tfrecords_path) and os.path.exists(val_tfrecords_path)):\n",
    "        print('data have already processed')\n",
    "        return train_num, val_num\n",
    "    \n",
    "    writer_train = tf.python_io.TFRecordWriter(train_tfrecords_path)\n",
    "    writer_val = tf.python_io.TFRecordWriter(val_tfrecords_path)\n",
    "    \n",
    "    labels = pd.read_csv(data_path + '/trainLabels.csv')\n",
    "    \n",
    "    pbar = ProgressBar().start()\n",
    "    for i, file_path in enumerate(files):\n",
    "        pbar.update(int((i / (total - 1)) * 100))#进度条\n",
    "        time.sleep(0.01)\n",
    "\n",
    "        img_raw = tf.gfile.FastGFile(file_path, 'rb').read()\n",
    "        label = label2int[labels.loc[i].label]\n",
    "\n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'img_raw': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw])),\n",
    "            'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\n",
    "        }))\n",
    "        if i < train_num:\n",
    "            writer_train.write(example.SerializeToString())\n",
    "        else:\n",
    "            writer_val.write(example.SerializeToString())\n",
    "    writer_train.close()\n",
    "    writer_val.close()\n",
    "    pbar.finish()\n",
    "    \n",
    "    return train_num, val_num\n",
    "def TFRecord2TrainData2(train_tfrecords_path, num_class, batch_size, num_epochs, shuffle=True):\n",
    "    filename_queue = tf.train.string_input_producer([train_tfrecords_path], num_epochs=num_epochs)\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)   #返回文件名和文件\n",
    "    features = tf.parse_single_example(serialized_example,\n",
    "                                   features={\n",
    "                                       'img_raw' : tf.FixedLenFeature([], tf.string),\n",
    "                                       'label': tf.FixedLenFeature([], tf.int64)\n",
    "                                   })\n",
    "    img = tf.image.decode_png(features['img_raw'])  # 与方式一的不同点在于需要用decode_png/decode_jpeg解码\n",
    "                                                    # output an RGB image. [height, width, channels]\n",
    "    img = tf.reshape(img, [32, 32, 3])\n",
    "    img = tf.image.resize_images(img, [224, 224])\n",
    "    label = tf.cast(features['label'], tf.int32)\n",
    "    label = tf.one_hot(label,num_class,1,0)\n",
    "    if shuffle==True:\n",
    "        img_batch, label_batch = tf.train.shuffle_batch([img, label], batch_size=batch_size,\\\n",
    "                                                        capacity=500 + 3 * batch_size, min_after_dequeue=500)\n",
    "    else:\n",
    "        img_batch, label_batch = tf.train.batch([img, label], batch_size=batch_size, capacity= 3 * batch_size)\n",
    "    return img_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data have already processed\n",
      "45000 5000\n"
     ]
    }
   ],
   "source": [
    "train_tfrecords_path = 'data/train.tfrecord'\n",
    "val_tfrecords_path = 'data/val.tfrecord'\n",
    "train_num, val_num = Trainset2TFRecord2(train_tfrecords_path, val_tfrecords_path)\n",
    "print(train_num, val_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "0 (128, 224, 224, 3)\n",
      "val\n",
      "0 (128, 224, 224, 3)\n",
      "train\n",
      "1 (128, 224, 224, 3)\n",
      "val\n",
      "1 (128, 224, 224, 3)\n",
      "train\n",
      "2 (128, 224, 224, 3)\n",
      "val\n",
      "2 (128, 224, 224, 3)\n",
      "train\n",
      "3 (128, 224, 224, 3)\n",
      "val\n",
      "3 (128, 224, 224, 3)\n",
      "train\n",
      "4 (128, 224, 224, 3)\n",
      "val\n",
      "4 (128, 224, 224, 3)\n",
      "train\n",
      "5 (128, 224, 224, 3)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#解码并查看效果\n",
    "#img,label = TFRecord2TrainData(train_tfrecords_path, 10)\n",
    "#img_batch, label_batch = tf.train.shuffle_batch([img, label], batch_size=500, capacity=50000, min_after_dequeue=1000, num_threads=1)\n",
    "#capacity是队列的最大容量, min_after_dequeue是dequeue后最小的队列大小,这个代表队列中的元素大于它的时候就输出乱的顺序的batch\n",
    "#num_threads是进行队列操作的线程数。\n",
    "img_train_batch, label_train_batch = TFRecord2TrainData2(train_tfrecords_path, 10, 128, 5)\n",
    "img_val_batch, label_val_batch = TFRecord2TrainData2(val_tfrecords_path, 10, 128, 5)#10类 batch_size=128, epoch = 5\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    #先产生！个看下效果\n",
    "    '''\n",
    "    for epoch in range(10):\n",
    "        try:\n",
    "            while not coord.should_stop():\n",
    "                img_, label_ = sess.run([img_batch, label_batch]) \n",
    "                print(epoch)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"done\")\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "        coord.join(threads)\n",
    "    ''' \n",
    "    #验证集上测试\n",
    "    try:\n",
    "        train_total_batch = int(train_num/128)\n",
    "        val_total_bath = int(val_num/128)\n",
    "        epoch = 0\n",
    "        while not coord.should_stop():#使用 coord.should_stop()来查询是否应该终止所有线程\n",
    "            #开始一个epoch的训练\n",
    "            train_begin = 0\n",
    "            val_begin = 0\n",
    "            for i in range(train_total_batch):\n",
    "                img_, label_ = sess.run([img_train_batch, label_train_batch]) \n",
    "                if train_begin == 0:\n",
    "                    print(\"train\")\n",
    "                    print(epoch, img_.shape)\n",
    "                    train_begin = 1\n",
    "            for i in range(val_total_bath):\n",
    "                img_, label_ = sess.run([img_val_batch, label_val_batch]) \n",
    "                if val_begin == 0:\n",
    "                    print(\"val\")\n",
    "                    print(epoch, img_.shape)\n",
    "                    val_begin = 1\n",
    "            epoch+=1\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"done\")\n",
    "    finally:       \n",
    "        coord.request_stop()#使用coord.request_stop()来发出终止所有线程的命令\n",
    "        coord.join(threads)#使用coord.join(threads)把线程加入主线程，等待threads结束。\n",
    "    '''\n",
    "    for i in range(10): #产生10个batch\n",
    "        img_, label_ = sess.run([img_batch, label_batch])#每次输出queue中的一个batch\n",
    "        plt.subplot(2, 5, i+1)\n",
    "        label = int2lable[np.argmax(label_, axis=1)[0]]\n",
    "        plt.title(label)\n",
    "        plt.imshow(img_[0].astype('uint8'))\n",
    "        i += 1\n",
    "    plt.show()\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conv5_4', 'conv5_1', 'fc6', 'conv5_3', 'conv5_2', 'conv4_4', 'fc7', 'conv4_1', 'conv4_2', 'conv4_3', 'fc8', 'conv3_4', 'conv3_3', 'conv3_2', 'conv3_1', 'conv1_1', 'conv1_2', 'conv2_2', 'conv2_1']\n",
      "(3, 3, 512, 512)\n",
      "(512,)\n"
     ]
    }
   ],
   "source": [
    "### 查看预训练模型中的参数 ###\n",
    "pre_model_path = 'model/vgg19/vgg19.npy'\n",
    "pre_model = np.load(pre_model_path, encoding = \"bytes\").item()\n",
    "layers = list(pre_model.keys())#将dict转换为list  层的名称\n",
    "#layer_1 = pre_model[layers[0]] #第一层中内容\n",
    "#layer_1_w =  pre_model[layers[0]][0]  #第一层的权重\n",
    "print(layers)\n",
    "layer_1_0 =  pre_model[layers[0]][1] #第一层的偏差\n",
    "print(pre_model[layers[0]][0].shape) #第一层权重的形状 (3, 3, 3, 64)\n",
    "print(pre_model[layers[0]][1].shape) #第一层的偏差的形状 (64,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_19:\n",
    "    def __init__(self, x, classname, is_training, weights=None, sess=None):\n",
    "        self.parameters = []\n",
    "        self.vgg_mean = [103.939, 116.779, 123.68]\n",
    "        self.build_model(x, classname, is_training)\n",
    "        self.skip = ['fc7', 'fc8'] #需要重新训练的层\n",
    "        self.probs = self.fc8\n",
    "        if weights is not None and sess is not None:\n",
    "            self.load_weights(weights, sess)\n",
    "    \n",
    "    def maxpool(self,name,input_data, trainable):\n",
    "        out = tf.nn.max_pool(input_data,[1,2,2,1],[1,2,2,1],padding=\"SAME\",name=name)\n",
    "        return out\n",
    "    \n",
    "    def conv(self,name, input_data, out_channel, trainable):\n",
    "        in_channel = input_data.get_shape()[-1]\n",
    "        with tf.variable_scope(name):\n",
    "            kernel = tf.get_variable(\"weights\", [3, 3, in_channel, out_channel], dtype=tf.float32,trainable=False)\n",
    "            biases = tf.get_variable(\"biases\", [out_channel], dtype=tf.float32,trainable=False)\n",
    "            conv_res = tf.nn.conv2d(input_data, kernel, [1, 1, 1, 1], padding=\"SAME\")\n",
    "            res = tf.nn.bias_add(conv_res, biases)\n",
    "            #norm = tf.layers.batch_normalization(res, center=True, scale=True, training=trainable)\n",
    "            out = tf.nn.relu(res, name=name)\n",
    "        self.parameters += [kernel, biases]\n",
    "        return out\n",
    "    \n",
    "    def fc(self,name,input_data,out_channel, relu_flag = True,trainable = True):\n",
    "        shape = input_data.get_shape().as_list()\n",
    "        if len(shape) == 4:\n",
    "            size = shape[-1] * shape[-2] * shape[-3]\n",
    "        else:size = shape[1]\n",
    "        input_data_flat = tf.reshape(input_data,[-1,size])\n",
    "        with tf.variable_scope(name):\n",
    "            weights = tf.get_variable(name=\"weights\",shape=[size,out_channel],dtype=tf.float32,trainable = trainable)\n",
    "            biases = tf.get_variable(name=\"biases\",shape=[out_channel],dtype=tf.float32,trainable = trainable)\n",
    "            res = tf.matmul(input_data_flat,weights)\n",
    "            if relu_flag == True:\n",
    "                #norm = tf.layers.batch_normalization(res, center=True, scale=True, training=trainable)\n",
    "                out = tf.nn.relu(tf.nn.bias_add(res,biases))\n",
    "            else:\n",
    "                out = res\n",
    "        self.parameters += [weights, biases]\n",
    "        return out\n",
    "    \n",
    "    def build_model(self, x, classnum, is_training):\n",
    "        # Preprocess\n",
    "        # Convert RGB to BGR opencv 读取的为BGR格式，所以不用转换\n",
    "        #red, green, blue = tf.split(axis=3, num_or_size_splits=3, value=self.rgb)#RGB格式\n",
    "        blue,  green, red = tf.split(axis=3, num_or_size_splits=3, value=x)#张量被切为三份\n",
    "        self.bgr = tf.concat(axis=3, values=[blue - self.vgg_mean[0], green - self.vgg_mean[1],red - self.vgg_mean[2]])\n",
    "        \n",
    "        # Block 1\n",
    "        #对于每个卷积层和全连接层中，不需要训练的权重全部被设置为trainable=False\n",
    "        self.conv1_1 = self.conv(\"conv1_1\",self.bgr,64,trainable=True)\n",
    "        self.conv1_2 = self.conv(\"conv1_2\",self.conv1_1,64,trainable=True)\n",
    "        self.pool1 = self.maxpool(\"pool1\",self.conv1_2,trainable=True)\n",
    "        \n",
    "        # Block 2\n",
    "        self.conv2_1 = self.conv(\"conv2_1\",self.pool1,128,trainable=True)\n",
    "        self.conv2_2 = self.conv(\"conv2_2\",self.conv2_1,128,trainable=True)\n",
    "        self.pool2 = self.maxpool(\"pool2\",self.conv2_2,trainable=True)  \n",
    "        \n",
    "        # Block 3\n",
    "        self.conv3_1 = self.conv(\"conv3_1\",self.pool2,256,trainable=True)\n",
    "        self.conv3_2 = self.conv(\"conv3_2\",self.conv3_1,256,trainable=True)\n",
    "        self.conv3_3 = self.conv(\"conv3_3\",self.conv3_2,256,trainable=True)\n",
    "        self.conv3_4 = self.conv(\"conv3_4\",self.conv3_3,256,trainable=True)\n",
    "        self.pool3 = self.maxpool(\"pool3\",self.conv3_4,trainable=True)  \n",
    "        \n",
    "        # Block 4\n",
    "        self.conv4_1 = self.conv(\"conv4_1\",self.pool3,512,trainable=True)\n",
    "        self.conv4_2 = self.conv(\"conv4_2\",self.conv4_1,512,trainable=True)\n",
    "        self.conv4_3 = self.conv(\"conv4_3\",self.conv4_2,512,trainable=True)\n",
    "        self.conv4_4 = self.conv(\"conv4_4\",self.conv4_3,512,trainable=True)\n",
    "        self.pool4 = self.maxpool(\"pool4\",self.conv4_4,trainable=True)\n",
    "        \n",
    "        # Block 5\n",
    "        self.conv5_1 = self.conv(\"conv5_1\",self.pool4,512,trainable=True)\n",
    "        self.conv5_2 = self.conv(\"conv5_2\",self.conv5_1,512,trainable=True)\n",
    "        self.conv5_3 = self.conv(\"conv5_3\",self.conv5_2,512,trainable=True)\n",
    "        self.conv5_4 = self.conv(\"conv5_4\",self.conv5_3,512,trainable=True)\n",
    "        self.pool5 = self.maxpool(\"pool5\",self.conv5_4,trainable=True)\n",
    "        \n",
    "        # model modification for cifar-10\n",
    "        self.fc6 = self.fc(\"fc6\", self.pool5, 4096, relu_flag=True, trainable=True)\n",
    "        if is_training == True:\n",
    "            self.fc6 = tf.layers.dropout(self.fc6, 0.5)\n",
    "        self.fc7 = self.fc(\"fc7\", self.fc6, 4096, relu_flag=True, trainable=True)\n",
    "        if is_training == True:\n",
    "            self.fc7 = tf.layers.dropout(self.fc7, 0.5)\n",
    "        self.fc8 = self.fc(\"fc8\", self.fc7, classnum, relu_flag=False, trainable=True)\n",
    "    \n",
    "    def load_npz_weights(self, weight_file, sess):\n",
    "        weights = np.load(weight_file)\n",
    "        keys = sorted(weights.keys())\n",
    "        for i, k in enumerate(keys):\n",
    "            if i not in [36,37]:#最后一层不参加训练\n",
    "                sess.run(self.parameters[i].assign(weights[k]))\n",
    "        print(\"-----------Load weights done!---------------\")\n",
    "    def load_npy_weights(self, weight_file, sess):\n",
    "        weights = np.load(weight_file, encoding = \"bytes\").item()\n",
    "        #for layers in model\n",
    "        for name in weights:\n",
    "            if name not in self.skip:\n",
    "                with tf.variable_scope(name, reuse = True):\n",
    "                    for p in weights[name]:\n",
    "                        if len(p.shape) == 1:\n",
    "                            #bias 只有一维\n",
    "                            sess.run(tf.get_variable('biases', trainable = True).assign(p))\n",
    "                        else:\n",
    "                            #weights\n",
    "                            sess.run(tf.get_variable('weights', trainable = True).assign(p)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 train loss is 0.491314 validataion loss is 0.512945 accuracy is 0.829928\n",
      "----  epoch 0 current best_validation_loss is 0.512945\n"
     ]
    }
   ],
   "source": [
    "### 用tfrecord训练 ###\n",
    "#训练参数\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 500             #迭代次数\n",
    "EARLY_STOP_PATIENCE = 10 #控制early stopping的参数\n",
    "NUM_CLASSS = 10\n",
    "\n",
    "tf.reset_default_graph()\n",
    "x_data = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "y_data = tf.placeholder(tf.float32, [None, 10])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "ckpt_path = './model/vgg/mode.ckpt'\n",
    "\n",
    "vgg = VGG_19(x_data, 10, is_training)\n",
    "predict = vgg.probs\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predict, labels=y_data))\n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):#批归一化层\n",
    "    train_step = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(predict, 1), tf.argmax(y_data, 1)), tf.float32))\n",
    "\n",
    "#从TFrecord中导入数据队列\n",
    "#img_batch,label_batch = TFRecord2TrainData2(train_tfrecords_path, NUM_CLASSS, BATCH_SIZE, EPOCHS)#每次读入1 epoch\n",
    "img_train_batch, label_train_batch = TFRecord2TrainData2(train_tfrecords_path, NUM_CLASSS, BATCH_SIZE, EPOCHS)\n",
    "img_val_batch, label_val_batch = TFRecord2TrainData2(val_tfrecords_path, NUM_CLASSS, BATCH_SIZE, EPOCHS)\n",
    "#img_batch, label_batch = tf.train.shuffle_batch([img, label], batch_size=BATCH_SIZE, capacity=50000, min_after_dequeue=500, num_threads=1)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "    vgg.load_npy_weights(pre_model_path, sess)\n",
    "\n",
    "    best_val_loss = 1000000.0\n",
    "    current_epoch = 0\n",
    "\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    \n",
    "    try:\n",
    "        train_total_batch = int(train_num/BATCH_SIZE)\n",
    "        val_total_bath = int(val_num/128)\n",
    "        epoch = 0\n",
    "        while not coord.should_stop() and epoch < EPOCHS:\n",
    "            #开始一个epoch的训练\n",
    "            train_loss_sum = 0\n",
    "            for i in range(train_total_batch):\n",
    "                x_train_batch, y_train_batch = sess.run([img_train_batch, label_train_batch])\n",
    "                _, train_loss_epoch = sess.run([train_step, loss], \\\n",
    "                             feed_dict={x_data:x_train_batch, y_data:y_train_batch, is_training:True})\n",
    "                train_loss_sum+=train_loss_epoch\n",
    "            train_loss = train_loss_sum/train_total_batch \n",
    "            #每经过一个epoch，在验证集上进行验证\n",
    "            val_acc_sum = 0\n",
    "            val_loss_sum = 0\n",
    "            for j in range(val_total_bath):\n",
    "                x_val_batch, y_val_batch = sess.run([img_val_batch, label_val_batch])\n",
    "                val_loss_epoch, val_acc_epoch = sess.run([loss, acc], feed_dict={x_data:x_val_batch, y_data:y_val_batch, is_training:False})\n",
    "                val_acc_sum+=val_acc_epoch\n",
    "                val_loss_sum+=val_loss_epoch\n",
    "            val_loss = val_loss_sum/val_total_bath\n",
    "            val_acc = val_acc_sum/val_total_bath\n",
    "            print(\"epoch %d train loss is %f validataion loss is %f accuracy is %f\" % (epoch, train_loss, val_loss, val_acc))\n",
    "           \n",
    "            if val_loss < best_val_loss:\n",
    "                print('----  epoch %d current best_validation_loss is %f' % (epoch, val_loss))\n",
    "                best_val_loss = val_loss\n",
    "                current_epoch = epoch\n",
    "                saver.save(sess, ckpt_path)\n",
    "            elif (epoch - current_epoch) >= EARLY_STOP_PATIENCE:\n",
    "                print('early stoping')\n",
    "                break  \n",
    "                \n",
    "            epoch += 1\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('done')\n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 计算混淆矩阵  ###\n",
    "def list_flatten(a):\n",
    "    b = []\n",
    "    for each in a:\n",
    "        if not isinstance(each, list):\n",
    "            b.append(each)\n",
    "        else:\n",
    "            b.extend(list_flatten(each))\n",
    "    return b  \n",
    "def testModel(ckpt_path):\n",
    "    tf.reset_default_graph()#mo\n",
    "    x_data = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "    y_data = tf.placeholder(tf.float32, [None, 10])\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    vgg = VGG_19(x_data, 10, is_training)\n",
    "    predict = vgg.probs\n",
    "    \n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(predict, 1), tf.argmax(y_data, 1)), tf.float32))\n",
    "    \n",
    "    img_batch,label_batch = TFRecord2TrainData2(val_tfrecords_path, 10, 128, 1, False)#每次读入1 epoch\n",
    "    \n",
    "    saver = tf.train.Saver() \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        saver.restore(sess, ckpt_path)\n",
    "        \n",
    "        acc = []\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        \n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)   \n",
    "        try:\n",
    "            while not coord.should_stop():\n",
    "                x_batch, y_batch = sess.run([img_batch, label_batch])\n",
    "                y_hat_batch = predict.eval({x_data: x_batch, is_training:False})\n",
    "                y_pred_batch = np.argmax(y_hat_batch, axis=1)\n",
    "                y_pred.append(y_pred_batch.tolist())\n",
    "                y_true_batch = np.argmax(y_batch, axis=1)\n",
    "                y_true.append(y_true_batch.tolist())\n",
    "                \n",
    "                acc_batch = accuracy.eval(feed_dict={x_data:x_batch, y_data:y_batch, is_training:False})\n",
    "                acc.append(acc_batch.tolist())        \n",
    "        \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('done')\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "            \n",
    "        y_true_array = np.array(list_flatten(y_true))\n",
    "        y_pred_array = np.array(list_flatten(y_pred))\n",
    "        cm = confusion_matrix(y_true_array, y_pred_array)\n",
    "        print(cm)\n",
    "        print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/vgg19/mode.ckpt\n",
      "done\n",
      "[[418   6  29  14   8   0   0   2  12   2]\n",
      " [  3 456   0   3   2   1   0   0   9  11]\n",
      " [ 12   1 433   4  11   3   8   5   1   0]\n",
      " [ 20   7  36 391   7  33   8  11   5   1]\n",
      " [ 13   0  77  16 332   3   7  22   4   1]\n",
      " [ 13   4  48  82  10 338   5  27   2   0]\n",
      " [  5   5  30  34   3   2 403   1   2   2]\n",
      " [  3   0  14   7  18   6   2 430   5   0]\n",
      " [ 66  17   3   5   8   0   2   2 417  12]\n",
      " [  3  27   1   1   0   0   0   0   4 475]]\n",
      "[0.8203125, 0.8046875, 0.828125, 0.75, 0.828125, 0.828125, 0.890625, 0.7890625, 0.796875, 0.8046875, 0.8515625, 0.78125, 0.8203125, 0.8671875, 0.8359375, 0.8125, 0.796875, 0.8125, 0.859375, 0.8203125, 0.8828125, 0.8125, 0.734375, 0.8515625, 0.875, 0.828125, 0.7890625, 0.828125, 0.8671875, 0.8203125, 0.8515625, 0.84375, 0.7734375, 0.859375, 0.7734375, 0.8125, 0.7578125, 0.8203125, 0.796875]\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = './model/vgg19/mode.ckpt'\n",
    "testModel(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 测试集数据的读取  ###\n",
    "#由于数据集较大，采用tensorflow自带的TFRecord进行读取\n",
    "def testset2TFRecord(test_tfrecords_path, data_path='data'):\n",
    "    files = natsorted(glob.glob(data_path + '/test/*'))\n",
    "    total = len(files)\n",
    "    if (os.path.exists(test_tfrecords_path)):\n",
    "        print('data have already processed')\n",
    "        return total\n",
    "    writer = tf.python_io.TFRecordWriter(test_tfrecords_path)\n",
    "\n",
    "    pbar = ProgressBar().start()\n",
    "    for i, file_path in enumerate(files):\n",
    "        pbar.update(int((i / (total - 1)) * 100))#进度条\n",
    "        time.sleep(0.01)\n",
    "        \n",
    "        img_raw = tf.gfile.FastGFile(file_path, 'rb').read()\n",
    "        example = tf.train.Example(features=tf.train.Features(feature={\n",
    "            'img_raw': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_raw]))\n",
    "        }))\n",
    "        writer.write(example.SerializeToString())\n",
    "    writer.close()\n",
    "    pbar.finish()\n",
    "def TFRecord2TestData(test_tfrecords_path, _batch_size=128):\n",
    "    filename_queue = tf.train.string_input_producer([test_tfrecords_path], num_epochs=1)\n",
    "    #第二个参数num_epochs: 可选参数，是一个整数值，代表迭代的次数，\n",
    "    #如果设置 num_epochs=None,生成器可以无限次遍历tensor列表，如果设置为 num_epochs=N，生成器只能遍历tensor列表N次。\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)   #返回文件名和文件\n",
    "    features = tf.parse_single_example(serialized_example,\n",
    "                                   features={\n",
    "                                       'img_raw' : tf.FixedLenFeature([], tf.string)\n",
    "                                   })\n",
    "    img = tf.image.decode_png(features['img_raw'])  # 与方式一的不同点在于需要用decode_png/decode_jpeg解码\n",
    "                                                    # output an RGB image. [height, width, channels]\n",
    "    img = tf.reshape(img, [32, 32, 3])\n",
    "    img = tf.image.resize_images(img, [224, 224])\n",
    "    img_batch = tf.train.batch([img], batch_size=_batch_size, capacity=3*_batch_size)\n",
    "    return img_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data have already processed\n",
      "300000\n"
     ]
    }
   ],
   "source": [
    "test_tfrecords_path = 'data/test.tfrecord'\n",
    "test_num = testset2TFRecord(test_tfrecords_path)\n",
    "print(test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Submit ###\n",
    "def submit(model_save_path, test_tfrecords_path, output_file):\n",
    "    tf.reset_default_graph()\n",
    "    x_test_data = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    vgg = VGG_19(x_test_data, 10, is_training)\n",
    "    predict = vgg.probs\n",
    "    \n",
    "    batch_size = 128\n",
    "    img_batch = TFRecord2TestData(test_tfrecords_path, batch_size)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        saver.restore(sess, model_save_path)\n",
    "\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess, coord)\n",
    "\n",
    "        f = open(output_file, 'w')\n",
    "        f.write('id,label\\n')\n",
    "        idx = 0\n",
    "        try:\n",
    "            pbar = ProgressBar().start()\n",
    "            while not coord.should_stop():\n",
    "                pbar.update(int((idx / (test_num - 1)) * 100))#进度条\n",
    "                time.sleep(0.01)\n",
    "                # just plot one batch size\n",
    "                x_test_batch = sess.run(img_batch)\n",
    "                y_hat = predict.eval({x_test_data: x_test_batch, is_training:False})\n",
    "                y_pred_batch = np.argmax(y_hat, axis=1)\n",
    "                for j in range(len(y_pred_batch)):\n",
    "                    f.write(\"\".join([str(idx+j), ',', int2lable[y_pred_batch[j]], '\\n']))                \n",
    "                idx+=len(y_pred_batch)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('done!')\n",
    "            pbar.finish()\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "        f.close()\n",
    "    print('Finish!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/vgg19/mode.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n",
      "Finish!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output_file = \"submission.csv\"\n",
    "submit(ckpt_path, test_tfrecords_path, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
