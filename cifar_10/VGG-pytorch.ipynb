{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F #torch是关于运算的包\n",
    "import torchvision\n",
    "from torchvision import datasets,transforms, models #torchvision则是打包了一些数据集\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 准备数据 ###\n",
    "def prepare_data():\n",
    "    print('==> Preparing data..')\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data/data_pytorch', train=True, download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=50, shuffle=True, num_workers=2)\n",
    "    valset = torchvision.datasets.CIFAR10(root='./data/data_pytorch', train=False, download=True, transform=transform_test)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=50, shuffle=False, num_workers=2)\n",
    "    return trainloader, valloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainloader, valloader = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 定义网络 ###\n",
    "cfg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = VGG('VGG16')\n",
    "#print(net)\n",
    "x = torch.randn(10,3,32,32)\n",
    "y = net(x)\n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace)\n",
       "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU(inplace)\n",
       "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU(inplace)\n",
       "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (19): ReLU(inplace)\n",
       "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU(inplace)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): ReLU(inplace)\n",
       "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (29): ReLU(inplace)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (32): ReLU(inplace)\n",
       "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (36): ReLU(inplace)\n",
       "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (39): ReLU(inplace)\n",
       "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (42): ReLU(inplace)\n",
       "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (44): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
       "  )\n",
       "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, device, trainloader, n_epoches):\n",
    "    '''Train our model.'''\n",
    "    # define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "    for epoch in range(n_epoches):\n",
    "        print('Epoch: %d' % epoch)\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, data_batch in enumerate(trainloader):\n",
    "            inputs, labels = data_batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimze\n",
    "            outputs = net(inputs)\n",
    "            print(outputs)\n",
    "            print(labels)\n",
    "            return\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print statistics\n",
    "            train_loss += loss.item()\n",
    "            _, predict = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predict.eq(labels).sum().item()\n",
    "            if i%100 == 99: # print every 100 batches\n",
    "                print('Loss: %.3f | Acc: %.3f %% (%d/%d)' % (train_loss/100,\n",
    "                    100*correct/total, correct, total))\n",
    "                train_loss = 0.0\n",
    "    print('Finish Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "tensor([[ -0.7631,  -4.0398,  -0.7474,  -1.7073,  16.5885,  -2.9221,   0.2001,\n",
      "           1.8999,  -5.5469,  -1.9875],\n",
      "        [ -3.7191,   0.1116,   2.6076,  -2.1457,  -2.3835,  -1.8586,  17.3826,\n",
      "          -5.4249,  -3.5935,   0.2269],\n",
      "        [ -3.3841,  -4.0257,   2.5052,  -1.0955,   8.0843,   2.4831,  -1.7781,\n",
      "           4.2227,  -2.5076,  -3.6173],\n",
      "        [  6.3324,  -1.3585,  -4.7995,  -1.5299,  -3.6718,  -3.1886, -10.0868,\n",
      "          -0.5161,  10.9555,   8.0773],\n",
      "        [  6.6694,  23.0939,  -9.1996, -10.1218,  -5.0884,  -6.2984,  -5.1723,\n",
      "          -4.2948,   4.4596,   6.0197],\n",
      "        [ -3.4369,  -1.7692,   4.3821,  -0.2962,  -0.5532,   9.4958,   0.8337,\n",
      "           2.2067,  -5.0549,  -4.8667],\n",
      "        [ -1.3537,  -1.7984,  -2.7713,   1.8624,  -1.9598,  -1.1147,  16.4356,\n",
      "          -2.5513,  -7.1465,   0.5658],\n",
      "        [ -0.8317,   4.4746,  -6.3499,  -6.3717,  -6.4665,  -3.7841,  -3.1704,\n",
      "           0.3358,   2.1499,  19.6901],\n",
      "        [ -2.4719,  23.4271, -13.6556,  -3.3780,  -7.6892,   0.5159,  -6.4832,\n",
      "          -0.5949,   2.4141,   8.0183],\n",
      "        [  6.9750,   1.7976,  -2.2745,  -2.1647,  -4.3242,  -4.7371,  -4.9162,\n",
      "          -4.2813,  15.3055,  -0.6365],\n",
      "        [  4.5291,   4.4598,  -5.7224,  -5.2264,  -6.8168,  -5.2211,  -7.5167,\n",
      "          -3.5301,  22.6855,   2.9987],\n",
      "        [  0.9652,  -0.0449,  -5.1832,  -5.0270,  -1.4059,  -9.3202,  -2.8451,\n",
      "          -1.8815,   4.9855,  20.2303],\n",
      "        [  0.4709,  -3.3579,  11.9582,   1.0488,  -3.0850,   1.8945,  -2.9604,\n",
      "           0.0358,  -0.5502,  -3.6635],\n",
      "        [ -0.1908,  -1.2604,   2.2631,  -2.4982,  -1.8969,  11.7490,  -3.3646,\n",
      "           3.1100,  -3.6566,  -3.3835],\n",
      "        [  7.1401,   4.2612,  -0.2471,  -2.9463,  -5.2242,  -2.3413,  -9.1000,\n",
      "          -1.7833,   0.8990,   9.5117],\n",
      "        [ -4.6315,  -2.7343,  -0.2636,  -0.5237,   0.5849,  20.3607,   0.3319,\n",
      "           0.1595,  -5.6294,  -6.0452],\n",
      "        [ -5.3973,  -5.5799,  -3.0839,  13.3867,   0.9218,   2.5320,   0.2497,\n",
      "          -0.3590,  -0.8114,  -1.5139],\n",
      "        [  5.8800,  -0.6953,  -3.6478,  -1.1927,  -6.7732,  -2.8939,  -7.6959,\n",
      "          -2.0333,  17.5872,   2.2123],\n",
      "        [  0.2121,   0.7093,  -0.7848,  -1.5131,  -5.1192,  -4.9863,  14.8323,\n",
      "          -2.7229,  -2.6971,   2.3626],\n",
      "        [  1.2927,  -3.3528,  14.5390,  -4.2731,  -1.8688,   0.6455,  -0.1851,\n",
      "           0.8927,  -2.3064,  -3.4849],\n",
      "        [  4.5461,   3.3102,  -2.5318,  -2.6749,  -7.7111,  -5.2354,  -4.5423,\n",
      "          -5.0562,  18.6656,   2.1696],\n",
      "        [ -1.5538,   3.2902,  -9.1146,  -7.3513,  -2.0603,  -7.8637,  -1.9235,\n",
      "          -0.2254,   2.4349,  24.1485],\n",
      "        [  3.7212,   1.4677,  -4.9824,  -4.8903,  -3.2073,  -2.6369,  -2.7942,\n",
      "          -3.8779,   0.7190,  17.0172],\n",
      "        [  8.8675,  -6.8192,   2.0999,   1.2246,  -3.8716,  -1.5760,  -2.5176,\n",
      "           0.3808,   3.4694,  -0.9250],\n",
      "        [ -0.7990,  -5.7315,   0.8372,   2.7697,  13.2498,   1.9092,   1.8513,\n",
      "          -1.7884,  -5.0568,  -6.1634],\n",
      "        [ -0.0321,  -3.9398,   2.9774,  -1.4579,  14.2935,  -2.0125,  -0.4920,\n",
      "          -0.7320,  -2.8660,  -4.3970],\n",
      "        [ -1.1168,  -3.0552,   1.3605,   0.9248,  -1.2788,  -6.0006,  17.9061,\n",
      "          -7.2650,   0.0072,  -0.6496],\n",
      "        [  0.4195,  -4.2527,   1.8363,  -0.4652,   9.7737,   0.5832,  -3.7787,\n",
      "           3.1109,  -2.5016,  -3.8345],\n",
      "        [  1.5477,  -3.7837,   0.9996,  -1.8215,  12.0946,  -0.2769,  -2.1932,\n",
      "           0.0801,  -2.6560,  -2.6817],\n",
      "        [ -3.1865,  -3.3713,  -1.2678,  11.5781,  -3.4614,   1.0485,  -1.2282,\n",
      "          -1.5834,   3.8102,  -1.3637],\n",
      "        [ -2.0366,  -3.5511,  -2.5328,  -0.8369,  15.5273,   1.7732,  -2.5958,\n",
      "           2.2717,  -4.6179,  -2.1176],\n",
      "        [ -3.4767,  -5.1973,   0.4771,   0.7315,  17.4945,   3.3026,   0.4758,\n",
      "           0.6947,  -6.8611,  -6.1865],\n",
      "        [  0.6045,  21.6360, -13.0900,  -0.0690,  -5.9709,  -3.1020,  -2.1329,\n",
      "          -5.5513,   2.1586,   5.8958],\n",
      "        [  0.2534,   0.8558,  -5.5216,  -4.0681,  -2.0443,   6.1176,  -7.0823,\n",
      "          15.7012,  -4.3197,  -1.1899],\n",
      "        [  0.1050,   9.0250,  -4.9037,   1.4960,  -1.7491,   3.0444,  -3.2054,\n",
      "          -3.5516,  -2.2485,   2.8948],\n",
      "        [ -3.2675,  -0.8297,  -1.7307,   2.4156,  -2.8633,  -3.2161,  11.7401,\n",
      "          -0.2813,  -3.3745,   1.2240],\n",
      "        [  3.5321,   0.9468,  -6.5031,   0.0028,  -1.4908,  -0.2381,  -6.0075,\n",
      "          10.8039,  -6.0915,   3.9220],\n",
      "        [ -4.9780,   0.6105,  -3.9501,  -0.2073,   1.1818,  -0.1536,  12.6506,\n",
      "          -2.5488,  -4.7983,   2.3738],\n",
      "        [ -5.0960,  -3.6303,   1.4286,   2.3151,  -0.6784,  -2.1465,  17.6357,\n",
      "          -4.4018,  -3.5133,  -1.2481],\n",
      "        [ 13.0817,  -4.4223,   5.4094,   1.4745,  -2.3512,  -4.5661,  -4.3104,\n",
      "          -2.1616,   1.0816,  -2.9772],\n",
      "        [  0.0281,   2.8021,  -2.3213,  -3.4887,  -3.9277,  -3.7464,  -3.5325,\n",
      "           0.6118,   1.4153,  12.2966],\n",
      "        [ -0.9866,  -5.2847,   9.5458,   0.2511,   1.0139,   2.5886,  -2.9799,\n",
      "           2.0401,  -1.2169,  -3.3866],\n",
      "        [ -2.3576,  -5.0316,  -0.9949,   4.5419,   8.3373,   1.5733,  -4.8243,\n",
      "           4.0410,  -2.9265,  -1.6619],\n",
      "        [ -3.0202,   5.1551,  -0.4311,  -0.1761,  -7.2254,  -2.6799,  11.7078,\n",
      "          -3.9497,  -1.5229,   2.6650],\n",
      "        [  5.9786,  -1.4195,  -1.3987,  -0.1593,  -3.4444,  -4.7513,  -5.2550,\n",
      "          -2.8206,  13.2541,   0.8422],\n",
      "        [ -4.3439,  -1.4394,   1.6296,  -2.2629,  -1.8519,  -6.0624,  19.4711,\n",
      "          -3.6961,  -1.9070,   1.1548],\n",
      "        [ -3.0651,  -6.0384,   3.2147,  10.2821,   3.5084,   1.5750,   3.7157,\n",
      "          -1.4626,  -5.2154,  -6.0660],\n",
      "        [ -0.0456,  -3.0279,  -0.6665,  -5.0703,   5.6786,  -1.4783,  -7.0076,\n",
      "          15.8950,  -4.3896,  -0.4435],\n",
      "        [ -2.9382,  -2.7019,  -2.8830,   4.4275,  -0.7849,   0.2713,  -1.9765,\n",
      "           8.1515,  -1.7700,  -0.2954],\n",
      "        [ -4.0266,   1.8729,  -0.9091,  -0.9942,  -4.6559,   0.3412,  16.4324,\n",
      "          -4.3507,  -4.3039,   1.2035]],\n",
      "       device='cuda:0', grad_fn=<ThAddmmBackward>)\n",
      "tensor([4, 6, 4, 8, 1, 5, 6, 9, 1, 8, 8, 9, 2, 5, 0, 5, 3, 8, 6, 2, 8, 9, 9, 0,\n",
      "        4, 4, 6, 4, 4, 3, 4, 4, 1, 7, 1, 6, 7, 6, 6, 0, 9, 2, 4, 6, 8, 6, 3, 7,\n",
      "        7, 6], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f1fc41532b0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lhw/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 399, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/lhw/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 378, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/home/lhw/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/lhw/anaconda3/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 151, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/lhw/anaconda3/lib/python3.6/multiprocessing/resource_sharer.py\", line 58, in detach\n",
      "    return reduction.recv_handle(conn)\n",
      "  File \"/home/lhw/anaconda3/lib/python3.6/multiprocessing/reduction.py\", line 182, in recv_handle\n",
      "    return recvfds(s, 1)[0]\n",
      "  File \"/home/lhw/anaconda3/lib/python3.6/multiprocessing/reduction.py\", line 153, in recvfds\n",
      "    msg, ancdata, flags, addr = sock.recvmsg(1, socket.CMSG_LEN(bytes_size))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    }
   ],
   "source": [
    "train(net, device, trainloader, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(net, device, valloader):\n",
    "    '''Compute the accuracy of our prediction on validation dataset.'''\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in valloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predict = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predict == labels).sum().item()\n",
    "    # print the accuracy\n",
    "    print('Accuracy of the network on the val images: %.3f %%' % (\n",
    "        100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the val images: 88.850 %\n"
     ]
    }
   ],
   "source": [
    "val(net, device, valloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Split: train\n",
      "    Root Location: ./data/data_pytorch\n",
      "    Transforms (if any): Compose(\n",
      "                             ToTensor()\n",
      "                             Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "                         )\n",
      "    Target Transforms (if any): None\n",
      "(50000, 32, 32, 3)\n",
      "<class 'list'> 50000\n"
     ]
    }
   ],
   "source": [
    "print(cifar_train)\n",
    "#print(cifar_test)\n",
    "print(cifar_train.train_data.shape)\n",
    "print(type(cifar_train.train_labels), len(cifar_train.train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "print(use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "for parma in model.parameters():\n",
    "    parma.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parma in model.parameters():\n",
    "    parma.requires_grad = False\n",
    "\n",
    "model.classifier = torch.nn.Sequential(torch.nn.Linear(25088, 4096),\n",
    "                                       torch.nn.ReLU(),\n",
    "                                       torch.nn.Dropout(p=0.5),\n",
    "                                       torch.nn.Linear(4096, 4096),\n",
    "                                       torch.nn.ReLU(),\n",
    "                                       torch.nn.Dropout(p=0.5),\n",
    "                                       torch.nn.Linear(4096, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在训练的时候我们可以自己写代码手动遍历数据集，指定batch和遍历方法，\n",
    "#不过PyTorch提供了一个DataLoader类来方便我们完成这些操作\n",
    "trainloader = torch.utils.data.DataLoader(cifar_train, batch_size=128, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(cifar_test, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "tensor([[[-0.1216, -0.1294, -0.1294,  ..., -0.1294, -0.1294, -0.1294],\n",
      "         [-0.1294, -0.1373, -0.1373,  ..., -0.1373, -0.1373, -0.1373],\n",
      "         [-0.1294, -0.1373, -0.1373,  ..., -0.1373, -0.1373, -0.1373],\n",
      "         ...,\n",
      "         [-0.1294, -0.1373, -0.1373,  ..., -0.1373, -0.1373, -0.1373],\n",
      "         [-0.1294, -0.1373, -0.1373,  ..., -0.1373, -0.1373, -0.1451],\n",
      "         [-0.1216, -0.1373, -0.1373,  ..., -0.1373, -0.1373, -0.1529]],\n",
      "\n",
      "        [[ 0.6078,  0.5922,  0.6000,  ...,  0.6000,  0.6000,  0.6000],\n",
      "         [ 0.5922,  0.5765,  0.5843,  ...,  0.5843,  0.5843,  0.5843],\n",
      "         [ 0.5922,  0.5843,  0.5843,  ...,  0.5843,  0.5843,  0.5843],\n",
      "         ...,\n",
      "         [ 0.5922,  0.5843,  0.5843,  ...,  0.5843,  0.5843,  0.5843],\n",
      "         [ 0.5922,  0.5843,  0.5843,  ...,  0.5843,  0.5843,  0.6000],\n",
      "         [ 0.5922,  0.5843,  0.5922,  ...,  0.5843,  0.5843,  0.5922]],\n",
      "\n",
      "        [[ 0.4902,  0.4745,  0.4824,  ...,  0.4824,  0.4824,  0.4824],\n",
      "         [ 0.4745,  0.4588,  0.4667,  ...,  0.4667,  0.4667,  0.4667],\n",
      "         [ 0.4745,  0.4667,  0.4667,  ...,  0.4667,  0.4667,  0.4667],\n",
      "         ...,\n",
      "         [ 0.4745,  0.4667,  0.4667,  ...,  0.4667,  0.4667,  0.4667],\n",
      "         [ 0.4745,  0.4667,  0.4667,  ...,  0.4667,  0.4667,  0.4667],\n",
      "         [ 0.4745,  0.4667,  0.4667,  ...,  0.4667,  0.4667,  0.4667]]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-3e75f62f6d7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-1e980a50c87e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, device, trainloader, n_epoches)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;31m# forward + backward + optimze\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision/models/vgg.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "train(model, 0, trainloader, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optim中定义了各种各样的优化方法，包括SGD\n",
    "import torch.optim as optim\n",
    "   \n",
    "if use_gpu:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    model = model.to(device)\n",
    "\n",
    "cost = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.classifier.parameters())\n",
    "\n",
    "print(\"Start Training...\")\n",
    "for epoch in range(30):\n",
    "    for param in [\"train\", \"val\"]:\n",
    "        if param == \"train\":\n",
    "            model.train = True\n",
    "        else:\n",
    "            model.train = False\n",
    "            \n",
    "        running_loss = 0.0\n",
    "        running_correct = 0 \n",
    "        batch = 0   \n",
    "    \n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device) # 注意需要复制到GPU\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss100 += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print('[Epoch %d, Batch %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, loss100 / 100))\n",
    "            loss100 = 0.0\n",
    "\n",
    "print(\"Done Training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "[Epoch 1, Batch   100] loss: 2.305\n",
      "[Epoch 1, Batch   200] loss: 2.306\n",
      "[Epoch 1, Batch   300] loss: 2.302\n",
      "[Epoch 1, Batch   400] loss: 2.302\n",
      "[Epoch 1, Batch   500] loss: 2.302\n",
      "[Epoch 1, Batch   600] loss: 2.302\n",
      "[Epoch 1, Batch   700] loss: 2.301\n",
      "[Epoch 1, Batch   800] loss: 2.300\n",
      "[Epoch 1, Batch   900] loss: 2.299\n",
      "[Epoch 1, Batch  1000] loss: 2.297\n",
      "[Epoch 1, Batch  1100] loss: 2.295\n",
      "[Epoch 1, Batch  1200] loss: 2.292\n",
      "[Epoch 1, Batch  1300] loss: 2.288\n",
      "[Epoch 1, Batch  1400] loss: 2.281\n",
      "[Epoch 1, Batch  1500] loss: 2.270\n",
      "[Epoch 2, Batch   100] loss: 2.220\n",
      "[Epoch 2, Batch   200] loss: 2.174\n",
      "[Epoch 2, Batch   300] loss: 2.114\n",
      "[Epoch 2, Batch   400] loss: 2.084\n",
      "[Epoch 2, Batch   500] loss: 2.067\n",
      "[Epoch 2, Batch   600] loss: 2.042\n",
      "[Epoch 2, Batch   700] loss: 1.988\n",
      "[Epoch 2, Batch   800] loss: 1.983\n",
      "[Epoch 2, Batch   900] loss: 1.962\n",
      "[Epoch 2, Batch  1000] loss: 1.963\n",
      "[Epoch 2, Batch  1100] loss: 1.897\n",
      "[Epoch 2, Batch  1200] loss: 1.890\n",
      "[Epoch 2, Batch  1300] loss: 1.865\n",
      "[Epoch 2, Batch  1400] loss: 1.837\n",
      "[Epoch 2, Batch  1500] loss: 1.795\n",
      "[Epoch 3, Batch   100] loss: 1.775\n",
      "[Epoch 3, Batch   200] loss: 1.728\n",
      "[Epoch 3, Batch   300] loss: 1.738\n",
      "[Epoch 3, Batch   400] loss: 1.736\n",
      "[Epoch 3, Batch   500] loss: 1.685\n",
      "[Epoch 3, Batch   600] loss: 1.677\n",
      "[Epoch 3, Batch   700] loss: 1.644\n",
      "[Epoch 3, Batch   800] loss: 1.667\n",
      "[Epoch 3, Batch   900] loss: 1.653\n",
      "[Epoch 3, Batch  1000] loss: 1.619\n",
      "[Epoch 3, Batch  1100] loss: 1.616\n",
      "[Epoch 3, Batch  1200] loss: 1.597\n",
      "[Epoch 3, Batch  1300] loss: 1.596\n",
      "[Epoch 3, Batch  1400] loss: 1.550\n",
      "[Epoch 3, Batch  1500] loss: 1.581\n",
      "[Epoch 4, Batch   100] loss: 1.531\n",
      "[Epoch 4, Batch   200] loss: 1.577\n",
      "[Epoch 4, Batch   300] loss: 1.549\n",
      "[Epoch 4, Batch   400] loss: 1.515\n",
      "[Epoch 4, Batch   500] loss: 1.492\n",
      "[Epoch 4, Batch   600] loss: 1.554\n",
      "[Epoch 4, Batch   700] loss: 1.491\n",
      "[Epoch 4, Batch   800] loss: 1.514\n",
      "[Epoch 4, Batch   900] loss: 1.514\n",
      "[Epoch 4, Batch  1000] loss: 1.498\n",
      "[Epoch 4, Batch  1100] loss: 1.483\n",
      "[Epoch 4, Batch  1200] loss: 1.515\n",
      "[Epoch 4, Batch  1300] loss: 1.470\n",
      "[Epoch 4, Batch  1400] loss: 1.440\n",
      "[Epoch 4, Batch  1500] loss: 1.462\n",
      "[Epoch 5, Batch   100] loss: 1.467\n",
      "[Epoch 5, Batch   200] loss: 1.420\n",
      "[Epoch 5, Batch   300] loss: 1.459\n",
      "[Epoch 5, Batch   400] loss: 1.438\n",
      "[Epoch 5, Batch   500] loss: 1.446\n",
      "[Epoch 5, Batch   600] loss: 1.439\n",
      "[Epoch 5, Batch   700] loss: 1.412\n",
      "[Epoch 5, Batch   800] loss: 1.410\n",
      "[Epoch 5, Batch   900] loss: 1.409\n",
      "[Epoch 5, Batch  1000] loss: 1.400\n",
      "[Epoch 5, Batch  1100] loss: 1.407\n",
      "[Epoch 5, Batch  1200] loss: 1.403\n",
      "[Epoch 5, Batch  1300] loss: 1.388\n",
      "[Epoch 5, Batch  1400] loss: 1.398\n",
      "[Epoch 5, Batch  1500] loss: 1.404\n",
      "[Epoch 6, Batch   100] loss: 1.381\n",
      "[Epoch 6, Batch   200] loss: 1.361\n",
      "[Epoch 6, Batch   300] loss: 1.380\n",
      "[Epoch 6, Batch   400] loss: 1.373\n",
      "[Epoch 6, Batch   500] loss: 1.355\n",
      "[Epoch 6, Batch   600] loss: 1.371\n",
      "[Epoch 6, Batch   700] loss: 1.345\n",
      "[Epoch 6, Batch   800] loss: 1.362\n",
      "[Epoch 6, Batch   900] loss: 1.314\n",
      "[Epoch 6, Batch  1000] loss: 1.340\n",
      "[Epoch 6, Batch  1100] loss: 1.335\n",
      "[Epoch 6, Batch  1200] loss: 1.317\n",
      "[Epoch 6, Batch  1300] loss: 1.337\n",
      "[Epoch 6, Batch  1400] loss: 1.325\n",
      "[Epoch 6, Batch  1500] loss: 1.339\n",
      "[Epoch 7, Batch   100] loss: 1.337\n",
      "[Epoch 7, Batch   200] loss: 1.293\n",
      "[Epoch 7, Batch   300] loss: 1.266\n",
      "[Epoch 7, Batch   400] loss: 1.307\n",
      "[Epoch 7, Batch   500] loss: 1.320\n",
      "[Epoch 7, Batch   600] loss: 1.257\n",
      "[Epoch 7, Batch   700] loss: 1.293\n",
      "[Epoch 7, Batch   800] loss: 1.288\n",
      "[Epoch 7, Batch   900] loss: 1.290\n",
      "[Epoch 7, Batch  1000] loss: 1.284\n",
      "[Epoch 7, Batch  1100] loss: 1.306\n",
      "[Epoch 7, Batch  1200] loss: 1.281\n",
      "[Epoch 7, Batch  1300] loss: 1.289\n",
      "[Epoch 7, Batch  1400] loss: 1.285\n",
      "[Epoch 7, Batch  1500] loss: 1.245\n",
      "[Epoch 8, Batch   100] loss: 1.257\n",
      "[Epoch 8, Batch   200] loss: 1.244\n",
      "[Epoch 8, Batch   300] loss: 1.241\n",
      "[Epoch 8, Batch   400] loss: 1.229\n",
      "[Epoch 8, Batch   500] loss: 1.241\n",
      "[Epoch 8, Batch   600] loss: 1.267\n",
      "[Epoch 8, Batch   700] loss: 1.223\n",
      "[Epoch 8, Batch   800] loss: 1.247\n",
      "[Epoch 8, Batch   900] loss: 1.252\n",
      "[Epoch 8, Batch  1000] loss: 1.235\n",
      "[Epoch 8, Batch  1100] loss: 1.214\n",
      "[Epoch 8, Batch  1200] loss: 1.249\n",
      "[Epoch 8, Batch  1300] loss: 1.235\n",
      "[Epoch 8, Batch  1400] loss: 1.248\n",
      "[Epoch 8, Batch  1500] loss: 1.211\n",
      "[Epoch 9, Batch   100] loss: 1.211\n",
      "[Epoch 9, Batch   200] loss: 1.203\n",
      "[Epoch 9, Batch   300] loss: 1.200\n",
      "[Epoch 9, Batch   400] loss: 1.194\n",
      "[Epoch 9, Batch   500] loss: 1.216\n",
      "[Epoch 9, Batch   600] loss: 1.194\n",
      "[Epoch 9, Batch   700] loss: 1.164\n",
      "[Epoch 9, Batch   800] loss: 1.216\n",
      "[Epoch 9, Batch   900] loss: 1.237\n",
      "[Epoch 9, Batch  1000] loss: 1.164\n",
      "[Epoch 9, Batch  1100] loss: 1.176\n",
      "[Epoch 9, Batch  1200] loss: 1.142\n",
      "[Epoch 9, Batch  1300] loss: 1.183\n",
      "[Epoch 9, Batch  1400] loss: 1.181\n",
      "[Epoch 9, Batch  1500] loss: 1.176\n",
      "[Epoch 10, Batch   100] loss: 1.151\n",
      "[Epoch 10, Batch   200] loss: 1.172\n",
      "[Epoch 10, Batch   300] loss: 1.157\n",
      "[Epoch 10, Batch   400] loss: 1.181\n",
      "[Epoch 10, Batch   500] loss: 1.174\n",
      "[Epoch 10, Batch   600] loss: 1.171\n",
      "[Epoch 10, Batch   700] loss: 1.159\n",
      "[Epoch 10, Batch   800] loss: 1.150\n",
      "[Epoch 10, Batch   900] loss: 1.168\n",
      "[Epoch 10, Batch  1000] loss: 1.155\n",
      "[Epoch 10, Batch  1100] loss: 1.142\n",
      "[Epoch 10, Batch  1200] loss: 1.136\n",
      "[Epoch 10, Batch  1300] loss: 1.167\n",
      "[Epoch 10, Batch  1400] loss: 1.120\n",
      "[Epoch 10, Batch  1500] loss: 1.100\n",
      "[Epoch 11, Batch   100] loss: 1.100\n",
      "[Epoch 11, Batch   200] loss: 1.112\n",
      "[Epoch 11, Batch   300] loss: 1.116\n",
      "[Epoch 11, Batch   400] loss: 1.100\n",
      "[Epoch 11, Batch   500] loss: 1.115\n",
      "[Epoch 11, Batch   600] loss: 1.135\n",
      "[Epoch 11, Batch   700] loss: 1.107\n",
      "[Epoch 11, Batch   800] loss: 1.109\n",
      "[Epoch 11, Batch   900] loss: 1.114\n",
      "[Epoch 11, Batch  1000] loss: 1.141\n",
      "[Epoch 11, Batch  1100] loss: 1.115\n",
      "[Epoch 11, Batch  1200] loss: 1.114\n",
      "[Epoch 11, Batch  1300] loss: 1.104\n",
      "[Epoch 11, Batch  1400] loss: 1.123\n",
      "[Epoch 11, Batch  1500] loss: 1.100\n",
      "[Epoch 12, Batch   100] loss: 1.066\n",
      "[Epoch 12, Batch   200] loss: 1.088\n",
      "[Epoch 12, Batch   300] loss: 1.084\n",
      "[Epoch 12, Batch   400] loss: 1.068\n",
      "[Epoch 12, Batch   500] loss: 1.099\n",
      "[Epoch 12, Batch   600] loss: 1.093\n",
      "[Epoch 12, Batch   700] loss: 1.102\n",
      "[Epoch 12, Batch   800] loss: 1.079\n",
      "[Epoch 12, Batch   900] loss: 1.070\n",
      "[Epoch 12, Batch  1000] loss: 1.084\n",
      "[Epoch 12, Batch  1100] loss: 1.071\n",
      "[Epoch 12, Batch  1200] loss: 1.063\n",
      "[Epoch 12, Batch  1300] loss: 1.077\n",
      "[Epoch 12, Batch  1400] loss: 1.058\n",
      "[Epoch 12, Batch  1500] loss: 1.096\n",
      "[Epoch 13, Batch   100] loss: 1.061\n",
      "[Epoch 13, Batch   200] loss: 1.041\n",
      "[Epoch 13, Batch   300] loss: 1.035\n",
      "[Epoch 13, Batch   400] loss: 1.076\n",
      "[Epoch 13, Batch   500] loss: 1.047\n",
      "[Epoch 13, Batch   600] loss: 1.065\n",
      "[Epoch 13, Batch   700] loss: 1.050\n",
      "[Epoch 13, Batch   800] loss: 1.043\n",
      "[Epoch 13, Batch   900] loss: 1.036\n",
      "[Epoch 13, Batch  1000] loss: 1.036\n",
      "[Epoch 13, Batch  1100] loss: 1.049\n",
      "[Epoch 13, Batch  1200] loss: 1.033\n",
      "[Epoch 13, Batch  1300] loss: 1.036\n",
      "[Epoch 13, Batch  1400] loss: 1.025\n",
      "[Epoch 13, Batch  1500] loss: 1.078\n",
      "[Epoch 14, Batch   100] loss: 0.993\n",
      "[Epoch 14, Batch   200] loss: 1.014\n",
      "[Epoch 14, Batch   300] loss: 1.013\n",
      "[Epoch 14, Batch   400] loss: 1.048\n",
      "[Epoch 14, Batch   500] loss: 1.037\n",
      "[Epoch 14, Batch   600] loss: 1.027\n",
      "[Epoch 14, Batch   700] loss: 1.075\n",
      "[Epoch 14, Batch   800] loss: 1.036\n",
      "[Epoch 14, Batch   900] loss: 1.015\n",
      "[Epoch 14, Batch  1000] loss: 0.997\n",
      "[Epoch 14, Batch  1100] loss: 0.997\n",
      "[Epoch 14, Batch  1200] loss: 1.020\n",
      "[Epoch 14, Batch  1300] loss: 1.004\n",
      "[Epoch 14, Batch  1400] loss: 0.995\n",
      "[Epoch 14, Batch  1500] loss: 1.017\n",
      "[Epoch 15, Batch   100] loss: 0.982\n",
      "[Epoch 15, Batch   200] loss: 0.956\n",
      "[Epoch 15, Batch   300] loss: 0.986\n",
      "[Epoch 15, Batch   400] loss: 1.026\n",
      "[Epoch 15, Batch   500] loss: 0.988\n",
      "[Epoch 15, Batch   600] loss: 0.964\n",
      "[Epoch 15, Batch   700] loss: 1.009\n",
      "[Epoch 15, Batch   800] loss: 0.982\n",
      "[Epoch 15, Batch   900] loss: 1.018\n",
      "[Epoch 15, Batch  1000] loss: 0.960\n",
      "[Epoch 15, Batch  1100] loss: 0.998\n",
      "[Epoch 15, Batch  1200] loss: 1.022\n",
      "[Epoch 15, Batch  1300] loss: 1.025\n",
      "[Epoch 15, Batch  1400] loss: 0.989\n",
      "[Epoch 15, Batch  1500] loss: 0.984\n",
      "[Epoch 16, Batch   100] loss: 0.969\n",
      "[Epoch 16, Batch   200] loss: 0.977\n",
      "[Epoch 16, Batch   300] loss: 0.920\n",
      "[Epoch 16, Batch   400] loss: 0.947\n",
      "[Epoch 16, Batch   500] loss: 0.968\n",
      "[Epoch 16, Batch   600] loss: 0.967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16, Batch   700] loss: 0.965\n",
      "[Epoch 16, Batch   800] loss: 0.973\n",
      "[Epoch 16, Batch   900] loss: 0.979\n",
      "[Epoch 16, Batch  1000] loss: 1.007\n",
      "[Epoch 16, Batch  1100] loss: 0.986\n",
      "[Epoch 16, Batch  1200] loss: 0.975\n",
      "[Epoch 16, Batch  1300] loss: 0.951\n",
      "[Epoch 16, Batch  1400] loss: 0.963\n",
      "[Epoch 16, Batch  1500] loss: 0.964\n",
      "[Epoch 17, Batch   100] loss: 0.931\n",
      "[Epoch 17, Batch   200] loss: 0.921\n",
      "[Epoch 17, Batch   300] loss: 0.929\n",
      "[Epoch 17, Batch   400] loss: 0.929\n",
      "[Epoch 17, Batch   500] loss: 0.981\n",
      "[Epoch 17, Batch   600] loss: 0.947\n",
      "[Epoch 17, Batch   700] loss: 0.946\n",
      "[Epoch 17, Batch   800] loss: 0.966\n",
      "[Epoch 17, Batch   900] loss: 0.937\n",
      "[Epoch 17, Batch  1000] loss: 0.969\n",
      "[Epoch 17, Batch  1100] loss: 0.961\n",
      "[Epoch 17, Batch  1200] loss: 0.942\n",
      "[Epoch 17, Batch  1300] loss: 0.907\n",
      "[Epoch 17, Batch  1400] loss: 0.948\n",
      "[Epoch 17, Batch  1500] loss: 0.948\n",
      "[Epoch 18, Batch   100] loss: 0.866\n",
      "[Epoch 18, Batch   200] loss: 0.907\n",
      "[Epoch 18, Batch   300] loss: 0.919\n",
      "[Epoch 18, Batch   400] loss: 0.914\n",
      "[Epoch 18, Batch   500] loss: 0.940\n",
      "[Epoch 18, Batch   600] loss: 0.915\n",
      "[Epoch 18, Batch   700] loss: 0.945\n",
      "[Epoch 18, Batch   800] loss: 0.921\n",
      "[Epoch 18, Batch   900] loss: 0.916\n",
      "[Epoch 18, Batch  1000] loss: 0.894\n",
      "[Epoch 18, Batch  1100] loss: 0.931\n",
      "[Epoch 18, Batch  1200] loss: 0.952\n",
      "[Epoch 18, Batch  1300] loss: 0.915\n",
      "[Epoch 18, Batch  1400] loss: 0.918\n",
      "[Epoch 18, Batch  1500] loss: 0.929\n",
      "[Epoch 19, Batch   100] loss: 0.884\n",
      "[Epoch 19, Batch   200] loss: 0.876\n",
      "[Epoch 19, Batch   300] loss: 0.890\n",
      "[Epoch 19, Batch   400] loss: 0.916\n",
      "[Epoch 19, Batch   500] loss: 0.863\n",
      "[Epoch 19, Batch   600] loss: 0.924\n",
      "[Epoch 19, Batch   700] loss: 0.872\n",
      "[Epoch 19, Batch   800] loss: 0.903\n",
      "[Epoch 19, Batch   900] loss: 0.890\n",
      "[Epoch 19, Batch  1000] loss: 0.907\n",
      "[Epoch 19, Batch  1100] loss: 0.920\n",
      "[Epoch 19, Batch  1200] loss: 0.891\n",
      "[Epoch 19, Batch  1300] loss: 0.900\n",
      "[Epoch 19, Batch  1400] loss: 0.933\n",
      "[Epoch 19, Batch  1500] loss: 0.898\n",
      "[Epoch 20, Batch   100] loss: 0.853\n",
      "[Epoch 20, Batch   200] loss: 0.873\n",
      "[Epoch 20, Batch   300] loss: 0.889\n",
      "[Epoch 20, Batch   400] loss: 0.882\n",
      "[Epoch 20, Batch   500] loss: 0.895\n",
      "[Epoch 20, Batch   600] loss: 0.887\n",
      "[Epoch 20, Batch   700] loss: 0.889\n",
      "[Epoch 20, Batch   800] loss: 0.896\n",
      "[Epoch 20, Batch   900] loss: 0.820\n",
      "[Epoch 20, Batch  1000] loss: 0.871\n",
      "[Epoch 20, Batch  1100] loss: 0.888\n",
      "[Epoch 20, Batch  1200] loss: 0.859\n",
      "[Epoch 20, Batch  1300] loss: 0.869\n",
      "[Epoch 20, Batch  1400] loss: 0.899\n",
      "[Epoch 20, Batch  1500] loss: 0.824\n",
      "[Epoch 21, Batch   100] loss: 0.821\n",
      "[Epoch 21, Batch   200] loss: 0.843\n",
      "[Epoch 21, Batch   300] loss: 0.820\n",
      "[Epoch 21, Batch   400] loss: 0.805\n",
      "[Epoch 21, Batch   500] loss: 0.824\n",
      "[Epoch 21, Batch   600] loss: 0.821\n",
      "[Epoch 21, Batch   700] loss: 0.842\n",
      "[Epoch 21, Batch   800] loss: 0.871\n",
      "[Epoch 21, Batch   900] loss: 0.876\n",
      "[Epoch 21, Batch  1000] loss: 0.878\n",
      "[Epoch 21, Batch  1100] loss: 0.883\n",
      "[Epoch 21, Batch  1200] loss: 0.863\n",
      "[Epoch 21, Batch  1300] loss: 0.891\n",
      "[Epoch 21, Batch  1400] loss: 0.868\n",
      "[Epoch 21, Batch  1500] loss: 0.899\n",
      "[Epoch 22, Batch   100] loss: 0.807\n",
      "[Epoch 22, Batch   200] loss: 0.818\n",
      "[Epoch 22, Batch   300] loss: 0.807\n",
      "[Epoch 22, Batch   400] loss: 0.831\n",
      "[Epoch 22, Batch   500] loss: 0.826\n",
      "[Epoch 22, Batch   600] loss: 0.835\n",
      "[Epoch 22, Batch   700] loss: 0.841\n",
      "[Epoch 22, Batch   800] loss: 0.824\n",
      "[Epoch 22, Batch   900] loss: 0.835\n",
      "[Epoch 22, Batch  1000] loss: 0.848\n",
      "[Epoch 22, Batch  1100] loss: 0.816\n",
      "[Epoch 22, Batch  1200] loss: 0.862\n",
      "[Epoch 22, Batch  1300] loss: 0.849\n",
      "[Epoch 22, Batch  1400] loss: 0.829\n",
      "[Epoch 22, Batch  1500] loss: 0.825\n",
      "[Epoch 23, Batch   100] loss: 0.771\n",
      "[Epoch 23, Batch   200] loss: 0.830\n",
      "[Epoch 23, Batch   300] loss: 0.781\n",
      "[Epoch 23, Batch   400] loss: 0.832\n",
      "[Epoch 23, Batch   500] loss: 0.801\n",
      "[Epoch 23, Batch   600] loss: 0.811\n",
      "[Epoch 23, Batch   700] loss: 0.810\n",
      "[Epoch 23, Batch   800] loss: 0.783\n",
      "[Epoch 23, Batch   900] loss: 0.816\n",
      "[Epoch 23, Batch  1000] loss: 0.822\n",
      "[Epoch 23, Batch  1100] loss: 0.820\n",
      "[Epoch 23, Batch  1200] loss: 0.807\n",
      "[Epoch 23, Batch  1300] loss: 0.798\n",
      "[Epoch 23, Batch  1400] loss: 0.835\n",
      "[Epoch 23, Batch  1500] loss: 0.869\n",
      "[Epoch 24, Batch   100] loss: 0.783\n",
      "[Epoch 24, Batch   200] loss: 0.756\n",
      "[Epoch 24, Batch   300] loss: 0.811\n",
      "[Epoch 24, Batch   400] loss: 0.807\n",
      "[Epoch 24, Batch   500] loss: 0.745\n",
      "[Epoch 24, Batch   600] loss: 0.766\n",
      "[Epoch 24, Batch   700] loss: 0.802\n",
      "[Epoch 24, Batch   800] loss: 0.755\n",
      "[Epoch 24, Batch   900] loss: 0.804\n",
      "[Epoch 24, Batch  1000] loss: 0.818\n",
      "[Epoch 24, Batch  1100] loss: 0.824\n",
      "[Epoch 24, Batch  1200] loss: 0.771\n",
      "[Epoch 24, Batch  1300] loss: 0.816\n",
      "[Epoch 24, Batch  1400] loss: 0.817\n",
      "[Epoch 24, Batch  1500] loss: 0.845\n",
      "[Epoch 25, Batch   100] loss: 0.718\n",
      "[Epoch 25, Batch   200] loss: 0.758\n",
      "[Epoch 25, Batch   300] loss: 0.758\n",
      "[Epoch 25, Batch   400] loss: 0.755\n",
      "[Epoch 25, Batch   500] loss: 0.763\n",
      "[Epoch 25, Batch   600] loss: 0.753\n",
      "[Epoch 25, Batch   700] loss: 0.764\n",
      "[Epoch 25, Batch   800] loss: 0.804\n",
      "[Epoch 25, Batch   900] loss: 0.787\n",
      "[Epoch 25, Batch  1000] loss: 0.821\n",
      "[Epoch 25, Batch  1100] loss: 0.780\n",
      "[Epoch 25, Batch  1200] loss: 0.756\n",
      "[Epoch 25, Batch  1300] loss: 0.804\n",
      "[Epoch 25, Batch  1400] loss: 0.763\n",
      "[Epoch 25, Batch  1500] loss: 0.784\n",
      "[Epoch 26, Batch   100] loss: 0.711\n",
      "[Epoch 26, Batch   200] loss: 0.703\n",
      "[Epoch 26, Batch   300] loss: 0.724\n",
      "[Epoch 26, Batch   400] loss: 0.743\n",
      "[Epoch 26, Batch   500] loss: 0.761\n",
      "[Epoch 26, Batch   600] loss: 0.787\n",
      "[Epoch 26, Batch   700] loss: 0.761\n",
      "[Epoch 26, Batch   800] loss: 0.744\n",
      "[Epoch 26, Batch   900] loss: 0.758\n",
      "[Epoch 26, Batch  1000] loss: 0.771\n",
      "[Epoch 26, Batch  1100] loss: 0.725\n",
      "[Epoch 26, Batch  1200] loss: 0.781\n",
      "[Epoch 26, Batch  1300] loss: 0.783\n",
      "[Epoch 26, Batch  1400] loss: 0.783\n",
      "[Epoch 26, Batch  1500] loss: 0.777\n",
      "[Epoch 27, Batch   100] loss: 0.729\n",
      "[Epoch 27, Batch   200] loss: 0.703\n",
      "[Epoch 27, Batch   300] loss: 0.719\n",
      "[Epoch 27, Batch   400] loss: 0.705\n",
      "[Epoch 27, Batch   500] loss: 0.721\n",
      "[Epoch 27, Batch   600] loss: 0.742\n",
      "[Epoch 27, Batch   700] loss: 0.696\n",
      "[Epoch 27, Batch   800] loss: 0.755\n",
      "[Epoch 27, Batch   900] loss: 0.719\n",
      "[Epoch 27, Batch  1000] loss: 0.781\n",
      "[Epoch 27, Batch  1100] loss: 0.778\n",
      "[Epoch 27, Batch  1200] loss: 0.765\n",
      "[Epoch 27, Batch  1300] loss: 0.734\n",
      "[Epoch 27, Batch  1400] loss: 0.743\n",
      "[Epoch 27, Batch  1500] loss: 0.764\n",
      "[Epoch 28, Batch   100] loss: 0.675\n",
      "[Epoch 28, Batch   200] loss: 0.680\n",
      "[Epoch 28, Batch   300] loss: 0.694\n",
      "[Epoch 28, Batch   400] loss: 0.716\n",
      "[Epoch 28, Batch   500] loss: 0.686\n",
      "[Epoch 28, Batch   600] loss: 0.739\n",
      "[Epoch 28, Batch   700] loss: 0.732\n",
      "[Epoch 28, Batch   800] loss: 0.726\n",
      "[Epoch 28, Batch   900] loss: 0.717\n",
      "[Epoch 28, Batch  1000] loss: 0.726\n",
      "[Epoch 28, Batch  1100] loss: 0.730\n",
      "[Epoch 28, Batch  1200] loss: 0.734\n",
      "[Epoch 28, Batch  1300] loss: 0.741\n",
      "[Epoch 28, Batch  1400] loss: 0.735\n",
      "[Epoch 28, Batch  1500] loss: 0.750\n",
      "[Epoch 29, Batch   100] loss: 0.681\n",
      "[Epoch 29, Batch   200] loss: 0.666\n",
      "[Epoch 29, Batch   300] loss: 0.692\n",
      "[Epoch 29, Batch   400] loss: 0.665\n",
      "[Epoch 29, Batch   500] loss: 0.677\n",
      "[Epoch 29, Batch   600] loss: 0.718\n",
      "[Epoch 29, Batch   700] loss: 0.679\n",
      "[Epoch 29, Batch   800] loss: 0.686\n",
      "[Epoch 29, Batch   900] loss: 0.697\n",
      "[Epoch 29, Batch  1000] loss: 0.726\n",
      "[Epoch 29, Batch  1100] loss: 0.712\n",
      "[Epoch 29, Batch  1200] loss: 0.732\n",
      "[Epoch 29, Batch  1300] loss: 0.732\n",
      "[Epoch 29, Batch  1400] loss: 0.722\n",
      "[Epoch 29, Batch  1500] loss: 0.698\n",
      "[Epoch 30, Batch   100] loss: 0.682\n",
      "[Epoch 30, Batch   200] loss: 0.650\n",
      "[Epoch 30, Batch   300] loss: 0.671\n",
      "[Epoch 30, Batch   400] loss: 0.655\n",
      "[Epoch 30, Batch   500] loss: 0.673\n",
      "[Epoch 30, Batch   600] loss: 0.673\n",
      "[Epoch 30, Batch   700] loss: 0.705\n",
      "[Epoch 30, Batch   800] loss: 0.665\n",
      "[Epoch 30, Batch   900] loss: 0.674\n",
      "[Epoch 30, Batch  1000] loss: 0.685\n",
      "[Epoch 30, Batch  1100] loss: 0.698\n",
      "[Epoch 30, Batch  1200] loss: 0.675\n",
      "[Epoch 30, Batch  1300] loss: 0.716\n",
      "[Epoch 30, Batch  1400] loss: 0.722\n",
      "[Epoch 30, Batch  1500] loss: 0.680\n",
      "Done Training!\n"
     ]
    }
   ],
   "source": [
    "# optim中定义了各种各样的优化方法，包括SGD\n",
    "import torch.optim as optim\n",
    "# CrossEntropyLoss就是我们需要的损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "print(\"Start Training...\")\n",
    "for epoch in range(30):\n",
    "    # 我们用一个变量来记录每100个batch的平均loss\n",
    "    loss100 = 0.0\n",
    "    # 我们的dataloader派上了用场\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device) # 注意需要复制到GPU\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss100 += loss.item()\n",
    "        if i % 100 == 99:\n",
    "            print('[Epoch %d, Batch %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, loss100 / 100))\n",
    "            loss100 = 0.0\n",
    "\n",
    "print(\"Done Training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 64 %\n"
     ]
    }
   ],
   "source": [
    "# 构造测试的dataloader\n",
    "dataiter = iter(testloader)\n",
    "# 预测正确的数量和总数量\n",
    "correct = 0\n",
    "total = 0\n",
    "# 使用torch.no_grad的话在前向传播中不记录梯度，节省内存\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # 预测\n",
    "        outputs = net(images)\n",
    "        # 我们的网络输出的实际上是个概率分布，去最大概率的哪一项作为预测分类\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
